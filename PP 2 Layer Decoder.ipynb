{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "jeIw3TKuSkFv",
    "outputId": "a663c6d3-eb0c-4b44-957a-25fabcf1fe10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  6 21:39:53 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   67C    P8    18W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fVNp3E1lSi1X",
    "outputId": "ccd75fb6-f16b-42be-895e-83001322bbb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "print('success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hK82MEcShtP"
   },
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar, Percentage, Bar\n",
    "from google.colab import files\n",
    "import torch, pickle, os, sys, random, time, math, copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jofhEDcZSZLH",
    "outputId": "c659e6f8-daea-44c1-fc19-d44f31b22b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906 281\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "########################### ENTIRE DATASET (WITH BPE) ###########################\n",
    "german = drive.CreateFile({'id': '1KNeZ_WQPUudwlJsg5_kUqPlLjJGBynsn'})\n",
    "german.GetContentFile('./german_bpe.pickle') \n",
    "with open('./german_bpe.pickle', 'rb') as f_in:\n",
    "    german = pickle.load(f_in)\n",
    "    \n",
    "english = drive.CreateFile({'id': '1ObXLocsuZsVGH3MVxSUdDBpiMlgBqVP7'})\n",
    "english.GetContentFile('./english_bpe.pickle') \n",
    "with open('./english_bpe.pickle', 'rb') as f_in:\n",
    "    english = pickle.load(f_in)\n",
    "    \n",
    "training_data = [[german['train'][i], english['train'][i]] for i in range(len(german['train']))]\n",
    "validation_data = [[german['dev'][i], english['dev'][i]] for i in range(len(german['dev']))]\n",
    "\n",
    "max_len_train = len(max(german['train'], key=len))\n",
    "max_len_valid = len(max(german['dev'], key=len))\n",
    "\n",
    "print(max_len_train, max_len_valid)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ji-pU6kFSVL4"
   },
   "outputs": [],
   "source": [
    "def to_padded_tensor(batch):\n",
    "    max_len_src = max([len(sent['source']) for sent in batch])  \n",
    "    max_len_trg = max([len(sent['target']) for sent in batch])\n",
    "    source_lengths = torch.zeros(len(batch), dtype=torch.int64).cpu()\n",
    "\n",
    "    for i, sent in enumerate(batch):\n",
    "        source_lengths[i] = len(sent['source'])\n",
    "        dif_src = max_len_src - len(sent['source'])\n",
    "        dif_trg = max_len_trg - len(sent['target'])\n",
    "\n",
    "        if dif_src > 0:\n",
    "            pad_list_src = [0 for d in range(dif_src)]\n",
    "            sent['source'].extend(pad_list_src)\n",
    "\n",
    "        if dif_trg > 0:\n",
    "            pad_list_trg = [0 for d in range(dif_trg)]\n",
    "            sent['target'].extend(pad_list_trg)       \n",
    "    \n",
    "    source_sent_len = max_len_src\n",
    "    target_sent_len = max_len_trg\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    source = torch.empty((source_sent_len, batch_size)).long().cpu()    \n",
    "    target = torch.empty((target_sent_len, batch_size)).long().cpu()\n",
    "    \n",
    "#     print(batch[0]['target'])\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        source[:,i] = torch.tensor(batch[i]['source'])\n",
    "        target[:,i] = torch.tensor(batch[i]['target'])\n",
    "        \n",
    "    padded_tensor = {\"source\": source.to(device),\n",
    "                    \"target\": target.to(device),\n",
    "                    \"srclen\": source_lengths}\n",
    "    \n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMxoSUgrSMiI"
   },
   "outputs": [],
   "source": [
    "def bake_batches(de, en, batch_size=1300, min_len=3, max_len=768, bucket_step=3):\n",
    "    german = copy.deepcopy(de)\n",
    "    english = copy.deepcopy(en)\n",
    "    \n",
    "    buckets = [[] for i in range(0, max_len, bucket_step)]\n",
    "    bucket_lengths = [0 for i in buckets]\n",
    "    batches = []\n",
    "    \n",
    "    # For every sentence in the dataset, find its corresponding bucket and put it in there, once the bucket\n",
    "    # hits the batch size, ship it off to the batches list\n",
    "    for i in range(len(german)):\n",
    "        sent2sent = {\"source\": german[i],\n",
    "                     \"target\": english[i]}\n",
    "        \n",
    "        # calculate the index of the buckets to put the sentence into, = len(Sentence) // Bucket_step - 1\n",
    "        idx = len(sent2sent['source'])//bucket_step - 1 \n",
    "\n",
    "        if bucket_lengths[idx] + len(sent2sent['source']) > batch_size:\n",
    "            sorted_bucket = sorted(buckets[idx], key=lambda x: len(x['source']), reverse=True)\n",
    "            batches.append(to_padded_tensor(sorted_bucket))\n",
    "            del buckets[idx][:]\n",
    "            buckets[idx].append(sent2sent)\n",
    "            bucket_lengths[idx] = len(sent2sent['source'])\n",
    "        else:\n",
    "            buckets[idx].append(sent2sent)\n",
    "            bucket_lengths[idx] += len(sent2sent['source'])\n",
    "            \n",
    "\n",
    "    # for any remaining buckets that did not get sent off, send them off to batches\n",
    "    for b in buckets:\n",
    "        if b: # if the list has any value in it\n",
    "            sorted_bucket = sorted(b, key=lambda x: len(x['source']), reverse=True)\n",
    "            batches.append(to_padded_tensor(sorted_bucket))\n",
    "            del b[:]\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GP2fk4oTR3_"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, num_layers=2, bidirectional = True, dropout=0.3)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #src_len = [src sent len]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
    "        \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                     \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "#         print(hidden.shape)\n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hiddenTOP = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        hiddenBOTTOM = torch.tanh(self.fc(torch.cat((hidden[-4,:,:], hidden[-3,:,:]), dim = 1)))\n",
    "        \n",
    "        hidden = torch.cat((hiddenBOTTOM.unsqueeze(0),hiddenTOP.unsqueeze(0)),dim=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #outputs = [sent len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znY3SLkkTTuH"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src sent len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "#         print(\"hidden\", hidden.shape)\n",
    "#         print(\"encoder out\", encoder_outputs.shape)\n",
    "        b = torch.cat((hidden, encoder_outputs), dim = 2)\n",
    "        energy = torch.tanh(self.attn(b)) \n",
    "\n",
    "#         energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src sent len, dec hid dim]\n",
    "                \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src sent len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "            \n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention = [batch size, src sent len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2k6c85_lTV13"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, num_layers=2, dropout=0.3)\n",
    "        \n",
    "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src sent len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "#         hidden = hidden[-1]\n",
    "#         print(\"Before attention:\",hidden.shape)\n",
    "        a = self.attention(hidden[-1], encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src sent len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src sent len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "#         print(hidden.shape)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden)#.unsqueeze(0))\n",
    "        \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "#         hidden = hidden[-1]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "#         assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0), a.squeeze(1)\n",
    "#         return output, hidden, a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMytQscpTXqn"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx, sos_idx, eos_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 1):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "#         print(\"Sentence Length:\", src.shape[0], \"Batch Size:\", src.shape[1])\n",
    "        \n",
    "        if trg is None:\n",
    "            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n",
    "            inference = True\n",
    "            trg = torch.zeros((100, src.shape[1])).long().fill_(self.sos_idx).to(src.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #tensor to store attention\n",
    "        attentions = torch.zeros(max_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "#         print(hidden.shape)\n",
    "                \n",
    "        #mask = [batch size, src sent len]\n",
    "                \n",
    "        for t in range(1, max_len):\n",
    "#             print(\"t\",t, hidden.shape)\n",
    "            output, hidden, attention = self.decoder(output, hidden, encoder_outputs, mask)\n",
    "#             print(hidden.shape)\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "            if inference and output.item() == self.eos_idx:\n",
    "                return outputs[:t], attentions[:t]\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7zaiWuLTaJ4"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch['source']\n",
    "        src_len = batch['srclen']\n",
    "        trg = batch['target']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attetion = model(src, src_len, trg)\n",
    "        if i%(len(iterator)//4) == 0: \n",
    "            print( \"batch:\", i, \",\", torch.cuda.memory_allocated(device)/1e6, \"MB used\") \n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-ZvNqg_TbqY"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src = batch['source']\n",
    "            src_len = batch['srclen']\n",
    "            trg = batch['target']\n",
    "\n",
    "            output, attention = model(src, src_len, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjlae7-bThBQ"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvRkeOcsTnQv"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmMFvmB4ToqX"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VUilz_SnVAkX",
    "outputId": "10c2325c-f053-452a-861b-793527767a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get an average batch size of 100 Use a batch_size value of: 2717\n"
     ]
    }
   ],
   "source": [
    "def average_sentence_size(german, desired_batch_size=128):\n",
    "    summation = 0\n",
    "    for i in german['train']:\n",
    "        summation += len(i)\n",
    "    avg_tok_sent = summation/len(german['train'])\n",
    "    \n",
    "    print(\"To get an average batch size of\", desired_batch_size, \"Use a batch_size value of:\", int(desired_batch_size*avg_tok_sent))\n",
    "    \n",
    "average_sentence_size(german, desired_batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "bx1wjCwnTrdn",
    "outputId": "cb8dcfa3-9ee4-4219-b803-98cad8fc488b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 39,368,227 trainable parameters\n",
      "Baking batches...\n",
      "Done.\n",
      "batch: 0 , 2575.153664 MB used\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "\n",
    "INPUT_DIM = len(german['idx2word'])\n",
    "OUTPUT_DIM = len(english['idx2word'])\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.4\n",
    "DEC_DROPOUT = 0.4\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "bad_epoch_cnt = 0\n",
    "\n",
    "results = {\"hyperparams\": (ENC_EMB_DIM, DEC_HID_DIM),\n",
    "           \"train_loss\": [],\n",
    "           \"valid_loss\": [],\n",
    "           \"train_ppl\": [],\n",
    "           \"valid_ppl\": []}\n",
    "\n",
    "valid_batches = bake_batches(german['dev'], english['dev'], batch_size=2400, max_len=max_len_valid)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    de_shuffled_td = [td[0] for td in training_data]\n",
    "    en_shuffled_td = [td[1] for td in training_data]\n",
    "\n",
    "    print(\"Baking batches...\")\n",
    "    train_batches = bake_batches(de_shuffled_td, en_shuffled_td, batch_size=2800, max_len=max_len_train)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_batches, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_batches, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if train_loss < best_train_loss:\n",
    "        torch.save(model.state_dict(), 'best-model-train.pt')\n",
    "\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        bad_epoch_cnt = 0\n",
    "        torch.save(model.state_dict(), 'best-model-val.pt')\n",
    "    else:\n",
    "        bad_epoch_cnt += 1  \n",
    "        \n",
    "    print(\"Validation loss has not improved in\", bad_epoch_cnt, \"epochs\")        \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    results['train_loss'].append(train_loss)\n",
    "    results['valid_loss'].append(valid_loss)\n",
    "    results['train_ppl'].append(math.exp(train_loss))\n",
    "    results['valid_ppl'].append(math.exp(valid_loss))\n",
    "    \n",
    "    if bad_epoch_cnt >= 5:\n",
    "        print(\"Early Stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPx-91qcYAR_"
   },
   "outputs": [],
   "source": [
    "def sample_net(net, batches):    \n",
    "    preds = []\n",
    "    targets = []\n",
    "    sources = []\n",
    "    sampled_batches = {}\n",
    "    \n",
    "       \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(batches):\n",
    "            src = batch['source']\n",
    "            src_len = batch['srclen']\n",
    "            trg = batch['target']\n",
    "\n",
    "            output, _ = net(src, src_len, trg, 0)\n",
    "            \n",
    "#             print(\"L length x N sentences:\", target.shape)\n",
    "#             print(type(output))\n",
    "#             print(\"L length x N sentences x V vocabulary:\", pred.shape)\n",
    "            pred = torch.argmax(output, dim=2)\n",
    "            \n",
    "#             print(\"L length x N sentences:\", pred.shape)\n",
    "#             print(\"L length x N sentences:\", target.shape)\n",
    "            \n",
    "            preds.append(pred)\n",
    "            targets.append(trg)\n",
    "            sources.append(src)\n",
    "    \n",
    "    sampled_batches['source'] = sources\n",
    "    sampled_batches['target'] = targets\n",
    "    sampled_batches['prediction'] = preds\n",
    "    \n",
    "    return sampled_batches\n",
    "# CLIP = 1\n",
    "\n",
    "# INPUT_DIM = len(german['idx2word'])\n",
    "# OUTPUT_DIM = len(english['idx2word'])\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "\n",
    "# attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "# enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "# dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "# model = Seq2Seq(enc, dec, device).to(device)\n",
    "# model.load_state_dict(torch.load(\"best-model.pt\"))\n",
    "\n",
    "batches = bake_batches(german['dev'], english['dev'])\n",
    "sampled_batches = sample_net(model, batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOMz7UZRqKb4"
   },
   "outputs": [],
   "source": [
    "def print_samples(batches):\n",
    "    # Each of the inputs should be a tensor batch of shape L x N\n",
    "    # print(sources.shape)\n",
    "    source = batches['source']\n",
    "    target = batches['target']\n",
    "    prediction = batches['prediction']\n",
    "    \n",
    "    sources_tokenized = []\n",
    "    targets_tokenized = []\n",
    "    preds_tokenized = []\n",
    "    \n",
    "#     md_de = MosesDetokenizer(lang='en')\n",
    "#     md_en = MosesDetokenizer(lang='en')\n",
    "\n",
    "\n",
    "    for b in range(len(batches['source'])):\n",
    "        for sent in range(source[b].shape[1]):\n",
    "            w = 1\n",
    "            src_sent = []\n",
    "            while w < target[b][:,sent].shape[0] and target[b][w,sent].item()!=3:\n",
    "                idx = target[b][w,sent].item()\n",
    "                src_sent.append(german['idx2word'][idx])\n",
    "                w+=1\n",
    "            sources_tokenized.append(src_sent)\n",
    "            \n",
    "            w = 1\n",
    "            tgt_sent = []\n",
    "            while w < target[b][:,sent].shape[0] and target[b][w,sent].item()!=3:\n",
    "                idx = target[b][w,sent].item()\n",
    "                tgt_sent.append(english['idx2word'][idx])\n",
    "                w+=1\n",
    "            targets_tokenized.append(tgt_sent)\n",
    "\n",
    "            w = 1\n",
    "            pred_sent = []\n",
    "            while w < prediction[b][:,sent].shape[0] and prediction[b][w,sent].item()!=3:\n",
    "                idx = prediction[b][w,sent].item()\n",
    "                pred_sent.append(english['idx2word'][idx])\n",
    "                w+=1\n",
    "            preds_tokenized.append(pred_sent)\n",
    "    \n",
    "    with open('source.out', 'w') as f:\n",
    "        for sent in sources_tokenized:\n",
    "            x = \" \".join(sent)\n",
    "            f.write(x + '\\n')\n",
    "    \n",
    "    with open('target.out', 'w') as f:\n",
    "        for sent in targets_tokenized:\n",
    "            x = \" \".join(sent)\n",
    "            f.write(x + '\\n')\n",
    "            \n",
    "    with open('pred.out', 'w') as f:\n",
    "        for sent in preds_tokenized:\n",
    "            y = \" \".join(sent)\n",
    "            f.write(y + '\\n')\n",
    "            \n",
    "print_samples(sampled_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OupV3yL-qPua"
   },
   "outputs": [],
   "source": [
    "!sed -r -i 's/(@@ )|(@@ ?$)//g' pred.out\n",
    "!sed -r -i 's/(@@ )|(@@ ?$)//g' source.out\n",
    "!sed -r -i 's/(@@ )|(@@ ?$)//g' target.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "L6QYqRnsqnHx",
    "outputId": "6bf7b8e4-7c68-4697-e1ee-108452a05b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mosesdecoder'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 147302 (delta 0), reused 0 (delta 0), pack-reused 147295\u001b[K\n",
      "Receiving objects: 100% (147302/147302), 129.69 MiB | 25.43 MiB/s, done.\n",
      "Resolving deltas: 100% (113834/113834), done.\n",
      "Collecting sacrebleu\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/dc/3ea54419418beb7d0ee0dedd08938519980b951419983a3ddd573c357190/sacrebleu-1.3.2.tar.gz\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
      "Building wheels for collected packages: sacrebleu\n",
      "  Building wheel for sacrebleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/f1/a9/4cce0ec602e8d195da27bb9b8f6708ec778fbafdbabb097fde\n",
      "Successfully built sacrebleu\n",
      "Installing collected packages: sacrebleu\n",
      "Successfully installed sacrebleu-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/moses-smt/mosesdecoder.git\n",
    "!pip install sacrebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "ILn097VIqgX5",
    "outputId": "8afbad17-38e1-490f-968b-9047c02b6f26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detokenizer Version $Revision: 4134 $\n",
      "Language: en\n",
      "BLEU+case.mixed+lang.de-en+numrefs.1+smooth.exp+tok.intl+version.1.3.2 = 17.5 53.9/25.6/13.7/7.4 (BP = 0.907 ratio = 0.911 hyp_len = 155812 ref_len = 171009)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "# !/bin/bash\n",
    "\n",
    "# This is a reference to the gold translations from the dev set\n",
    "REFERENCE_FILE=\"target.out\"\n",
    "\n",
    "# XXX: Change the following line to point to your model's output!\n",
    "TRANSLATED_FILE=\"pred.out\"\n",
    "\n",
    "# The model output is expected to be in a tokenized form. Note, that if you\n",
    "# tokenized your inputs to the model, then simply joined each output token with\n",
    "# whitespace you should get tokenized outputs from your model.\n",
    "# i.e. each output token is separate by whitespace\n",
    "# e.g. \"My model 's output is interesting .\"\n",
    "perl \"mosesdecoder/scripts/tokenizer/detokenizer.perl\" -l en < \"$TRANSLATED_FILE\" > \"$TRANSLATED_FILE.detok\"\n",
    "\n",
    "PARAMS=(\"-tok\" \"intl\" \"-l\" \"de-en\" \"$REFERENCE_FILE\")\n",
    "sacrebleu \"${PARAMS[@]}\" < \"$TRANSLATED_FILE.detok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LP9p44_1qjTR"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model-bleu-17.5.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHWzTddZrbjo"
   },
   "outputs": [],
   "source": [
    "a = torch.arange(12).reshape((3,4))\n",
    "b = torch.arange(20,32).reshape((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdXI5-2F-RND"
   },
   "outputs": [],
   "source": [
    "c = torch.cat((a.unsqueeze(0),b.unsqueeze(0)),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WxhsUIMs-yfY",
    "outputId": "0f5764ff-114a-4773-881a-1bd9dcad0a7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "VTTmueb9MG9w",
    "outputId": "21c5c7b2-75fa-4573-8425-78cf266a8cbb"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-92d7adb02c84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert (a == b).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPJZoE9BPY7V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Packed_Padded (1).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
