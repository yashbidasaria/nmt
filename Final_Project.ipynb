{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "F6YKpEtz2Wsk",
        "colab_type": "code",
        "outputId": "47ed52ee-4f4c-4ae1-f0ee-93628bbd884e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr 30 12:34:28 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ai-duA696vlS",
        "colab_type": "code",
        "outputId": "49d8987c-eb3f-4a46-8d59-e06de0b46ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print('success!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 26.5MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 31.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 35.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 39.2MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 43.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 43.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 44.7MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 46.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 47.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 47.9MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 47.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 47.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 47.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 47.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 47.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 47.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 47.9MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZiVfPF4F90za",
        "colab_type": "code",
        "outputId": "3906aa79-4e1b-4700-c141-91bed32c0b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "from progressbar import ProgressBar, Percentage, Bar\n",
        "from google.colab import files\n",
        "import torch, pickle, os, sys, random, time, math, copy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from queue import PriorityQueue\n",
        "\n",
        "########################### ENTIRE DATASET ###########################\n",
        "# german = drive.CreateFile({'id': '1I0_go5RhzEg2CkbpZgx8a6h4hA9qNeyY'})\n",
        "# german.GetContentFile('./german_no_pad_sorted.pickle') \n",
        "# with open('./german_no_pad_sorted.pickle', 'rb') as f_in:\n",
        "#     german = pickle.load(f_in)\n",
        "    \n",
        "# english = drive.CreateFile({'id': '1Tb0aYVfkj3YyXXTxWRkvDO7T__ovng1Q'})\n",
        "# english.GetContentFile('./english_no_pad_sorted.pickle') \n",
        "# with open('./english_no_pad_sorted.pickle', 'rb') as f_in:\n",
        "#     english = pickle.load(f_in)    \n",
        "\n",
        "########################### 5K DATASET ###########################   \n",
        "# german = drive.CreateFile({'id': '10epaM5VzSskc0xkriD0YN_a0Mg3-o1bg'})\n",
        "# german.GetContentFile('./german_no_pad_sorted_5k.pickle') \n",
        "# with open('./german_no_pad_sorted_5k.pickle', 'rb') as f_in:\n",
        "#     german = pickle.load(f_in)\n",
        "\n",
        "# english = drive.CreateFile({'id': '15P0N9sLAIPyxF0Uhgx-01P2s-xOLp6xb'})\n",
        "# english.GetContentFile('./english_no_pad_sorted_5k.pickle') \n",
        "# with open('./english_no_pad_sorted_5k.pickle', 'rb') as f_in:\n",
        "#     english = pickle.load(f_in)\n",
        "\n",
        "########################### 50K DATASET ###########################    \n",
        "# german = drive.CreateFile({'id': '1bNBrVR9TCNXRqlS9kxPa3EffYyZnAfCw'})\n",
        "# german.GetContentFile('./german_no_pad_sorted_50k.pickle') \n",
        "# with open('./german_no_pad_sorted_50k.pickle', 'rb') as f_in:\n",
        "#     german = pickle.load(f_in)\n",
        "    \n",
        "# english = drive.CreateFile({'id': '1XnCWgSLGcp3lCdUK0ciS5m6mwI5L5R2l'})\n",
        "# english.GetContentFile('./english_no_pad_sorted_50k.pickle') \n",
        "# with open('./english_no_pad_sorted_50k.pickle', 'rb') as f_in:\n",
        "#     english = pickle.load(f_in)\n",
        "\n",
        "########################### ENTIRE DATASET (WITH UNK) ###########################\n",
        "german = drive.CreateFile({'id': '1FuoqjfMofNfkIRXBIWDU_cIVV9BBWcs-'})\n",
        "german.GetContentFile('./german_unk.pickle') \n",
        "with open('./german_unk.pickle', 'rb') as f_in:\n",
        "    german = pickle.load(f_in)\n",
        "    \n",
        "english = drive.CreateFile({'id': '18Q-WRZ2JBwkAJLXwiVuWPfyE-0k76FDA'})\n",
        "english.GetContentFile('./english_unk.pickle') \n",
        "with open('./english_unk.pickle', 'rb') as f_in:\n",
        "    english = pickle.load(f_in)    \n",
        "\n",
        "# for i in range(len(german['train'])):\n",
        "#     german['train'][i] = torch.LongTensor(german['train'][i]).cuda()\n",
        "#     english['train'][i] = torch.LongTensor(english['train'][i]).cuda()\n",
        "    \n",
        "# for i in range(len(german['dev'])):\n",
        "#     german['dev'][i] = torch.LongTensor(german['dev'][i]).cuda()\n",
        "#     english['dev'][i] = torch.LongTensor(english['dev'][i]).cuda()\n",
        "\n",
        "training_data = [[german['train'][i], english['train'][i]] for i in range(len(german['train']))]\n",
        "validation_data = [[german['dev'][i], english['dev'][i]] for i in range(len(german['dev']))]\n",
        "\n",
        "max_len_train = len(max(german['train'], key=len))\n",
        "max_len_valid = len(max(german['dev'], key=len))\n",
        "\n",
        "print(max_len_train, max_len_valid)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768 169\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pxfBDiABDVNp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_padded_tensor(batch):\n",
        "    max_len_src = max([len(sent['source']) for sent in batch])  \n",
        "    max_len_trg = max([len(sent['target']) for sent in batch])\n",
        "\n",
        "    for sent in batch:\n",
        "        dif_src = max_len_src - len(sent['source'])\n",
        "        dif_trg = max_len_trg - len(sent['target'])\n",
        "\n",
        "        if dif_src > 0:\n",
        "            pad_list_src = [0 for d in range(dif_src)]\n",
        "            sent['source'].extend(pad_list_src)\n",
        "\n",
        "        if dif_trg > 0:\n",
        "            pad_list_trg = [0 for d in range(dif_trg)]\n",
        "            sent['target'].extend(pad_list_trg)       \n",
        "    \n",
        "    source_sent_len = max_len_src\n",
        "    target_sent_len = max_len_trg\n",
        "    \n",
        "    batch_size = len(batch)\n",
        "    \n",
        "    source = torch.empty((source_sent_len, batch_size)).long().cpu()    \n",
        "    target = torch.empty((target_sent_len, batch_size)).long().cpu()\n",
        "    \n",
        "#     print(batch[0]['target'])\n",
        "    \n",
        "    for i in range(len(batch)):\n",
        "        source[:,i] = torch.tensor(batch[i]['source'])\n",
        "        target[:,i] = torch.tensor(batch[i]['target'])\n",
        "        \n",
        "    padded_tensor = {\"source\": source.to(device),\n",
        "                    \"target\": target.to(device)}\n",
        "    \n",
        "    return padded_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQAoZvykDStp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bake_batches(de, en, batch_size=1300, min_len=3, max_len=768, bucket_step=3):\n",
        "    german = copy.deepcopy(de)\n",
        "    english = copy.deepcopy(en)\n",
        "    \n",
        "    buckets = [[] for i in range(0, max_len, bucket_step)]\n",
        "    bucket_lengths = [0 for i in buckets]\n",
        "    batches = []\n",
        "    \n",
        "    # For every sentence in the dataset, find its corresponding bucket and put it in there, once the bucket\n",
        "    # hits the batch size, ship it off to the batches list\n",
        "    for i in range(len(german)):\n",
        "        sent2sent = {\"source\": german[i],\n",
        "                     \"target\": english[i]}\n",
        "        \n",
        "        # calculate the index of the buckets to put the sentence into, = len(Sentence) // Bucket_step - 1\n",
        "        idx = len(sent2sent['source'])//bucket_step - 1\n",
        "        \n",
        "        if bucket_lengths[idx] + len(sent2sent['source']) > batch_size:\n",
        "            batches.append(to_padded_tensor(buckets[idx][:]))\n",
        "            del buckets[idx][:]\n",
        "            buckets[idx].append(sent2sent)\n",
        "            bucket_lengths[idx] = len(sent2sent['source'])\n",
        "        else:\n",
        "            buckets[idx].append(sent2sent)\n",
        "            bucket_lengths[idx] += len(sent2sent['source'])\n",
        "            \n",
        "\n",
        "    # for any remaining buckets that did not get sent off, send them off to batches\n",
        "    for b in buckets:\n",
        "        if b: # if the list has any value in it\n",
        "            batches.append(to_padded_tensor(b[:]))\n",
        "            del b[:]\n",
        "    \n",
        "    return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZX5hRdP2fHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src sent len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rL9QXs3xYoOn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, beam=False):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat encoder hidden state src_len times\n",
        "        if not beam:\n",
        "            hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        if not self.training and beam:\n",
        "            hidden = hidden.repeat(encoder_outputs.size(0),encoder_outputs.size(1), 1)\n",
        "        \n",
        "        #hidden = [batch size, src sent len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src sent len, dec hid dim]\n",
        "        \n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        \n",
        "        #energy = [batch size, dec hid dim, src sent len]\n",
        "        \n",
        "        #v = [dec hid dim]\n",
        "        \n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "        \n",
        "        #v = [batch size, 1, dec hid dim]\n",
        "                \n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoiZeHuP4fPW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, beam=False):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        if not beam: \n",
        "            input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [sent len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #output = [bsz, output dim]\n",
        "        \n",
        "        return output, hidden.squeeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mxmqAe9uKTN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "        \n",
        "    def beam_search(self, target_tensor, decoder_hiddens, encoder_outputs=None):\n",
        "        '''\n",
        "        :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
        "        :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
        "        :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
        "        :return: decoded_batch\n",
        "        '''\n",
        "\n",
        "        beam_width = 4\n",
        "        topk = 4 # how many sentence do you want to generate\n",
        "        decoded_batch = []\n",
        "        # decoding goes sentence by sentence\n",
        "        sentence_length, num_sentences = target_tensor.size()\n",
        "        for idx in range(num_sentences):\n",
        "            if isinstance(decoder_hiddens, tuple):  # LSTM case\n",
        "                decoder_hidden = (decoder_hiddens[0][:,idx, :].unsqueeze(0),decoder_hiddens[1][:,idx, :].unsqueeze(0))\n",
        "            else:\n",
        "                decoder_hidden = decoder_hiddens[idx, :].unsqueeze(0)\n",
        "            encoder_output = encoder_outputs[:,idx, :].unsqueeze(1)\n",
        "\n",
        "            # Start with the start of the sentence token\n",
        "            decoder_input = torch.LongTensor([[2]]).cuda()\n",
        "\n",
        "            # Number of sentence to generate\n",
        "            endnodes = []\n",
        "            number_required = min((topk + 1), topk - len(endnodes))\n",
        "\n",
        "            # starting node -  hidden vector, previous node, word id, logp, length\n",
        "            node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
        "            nodes = PriorityQueue()\n",
        "\n",
        "            # start the queue\n",
        "            nodes.put(node)\n",
        "            qsize = 1\n",
        "\n",
        "            # start beam search\n",
        "            while True:\n",
        "                # give up when decoding takes too long\n",
        "                if qsize > 10000: break\n",
        "\n",
        "                # fetch the best node\n",
        "                n = nodes.get()\n",
        "                decoder_input = n.wordid\n",
        "                decoder_hidden = n.h\n",
        "\n",
        "                if n.wordid.item() == 3 and n.prevNode != None:\n",
        "                    endnodes.append(n)\n",
        "                    # if we reached maximum # of sentences required\n",
        "                    if len(endnodes) >= number_required:\n",
        "                        break\n",
        "                    else:\n",
        "                        continue\n",
        "                \n",
        "                # decode for one step using decoder\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output, True)\n",
        "\n",
        "                # PUT HERE REAL BEAM SEARCH OF TOP\n",
        "                # take softmax first, then log\n",
        "                decoder_output = self.softmax(decoder_output)\n",
        "                \n",
        "                log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
        "                nextnodes = []\n",
        "\n",
        "                for new_k in range(beam_width):\n",
        "                    decoded_t = indexes[0][new_k].view(1, -1)\n",
        "                    log_p = log_prob[0][new_k].item()\n",
        "\n",
        "                    node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp - log_p, n.leng + 1)\n",
        "                    score = -node.eval()\n",
        "                    nextnodes.append(node)\n",
        "\n",
        "                # put them into queue\n",
        "                for i in range(len(nextnodes)):\n",
        "                    nn = nextnodes[i]\n",
        "                    nodes.put(nn)\n",
        "                    # increase qsize\n",
        "                qsize += len(nextnodes) - 1\n",
        "\n",
        "            # choose nbest paths, back trace them\n",
        "            if len(endnodes) == 0:\n",
        "                endnodes = [nodes.get() for _ in range(topk)]\n",
        "\n",
        "            utterances = []\n",
        "            max_prob = -100000\n",
        "            max_sen = False\n",
        "            final_sentence = []\n",
        "            for n in sorted(endnodes):\n",
        "                utterance = []\n",
        "                if max_prob < n.logp:\n",
        "                    max_prob = n.logp\n",
        "                    max_sen = True\n",
        "                utterance.append(n.wordid)\n",
        "                # back trace\n",
        "                while n.prevNode != None:\n",
        "                    n = n.prevNode\n",
        "                    utterance.append(n.wordid)\n",
        "                  \n",
        "                utterance = utterance[::-1]\n",
        "                if max_sen:\n",
        "                    max_sen = False\n",
        "                    final_sentence = utterance\n",
        "                utterances.append(utterance)\n",
        "                \n",
        "                \n",
        "            decoded_batch.append(torch.IntTensor(final_sentence))\n",
        "\n",
        "        return decoded_batch\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5, run_beam=False):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        #trg = [trg sent len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        output = trg[0,:]\n",
        "        \n",
        "        if run_beam:\n",
        "            temp = self.beam_search(trg, hidden ,encoder_outputs)\n",
        "            return temp\n",
        "        # else use greedy\n",
        "        \n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "emX_0uUlKC9m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BeamSearchNode(object):\n",
        "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
        "        '''\n",
        "        :param hiddenstate:\n",
        "        :param previousNode:\n",
        "        :param wordId:\n",
        "        :param logProb:\n",
        "        :param length:\n",
        "        '''\n",
        "        self.h = hiddenstate\n",
        "        self.prevNode = previousNode\n",
        "        self.wordid = wordId\n",
        "        self.logp = logProb\n",
        "        self.leng = length\n",
        "\n",
        "    def eval(self, alpha=1.0):\n",
        "        reward = 0\n",
        "        # Add here a function for shaping a reward\n",
        "\n",
        "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward\n",
        "      \n",
        "      \n",
        "    def __lt__(self, other):\n",
        "        return self.logp < other.logp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ejMfHYJc9io",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, batches, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, b in enumerate(batches):\n",
        "        \n",
        "        source = b['source']\n",
        "        target = b['target']\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(source, target)\n",
        "        \n",
        "        if i%(len(batches)//4) == 0: \n",
        "            print( \"batch:\", i, \",\", torch.cuda.memory_allocated(device)/1e6, \"MB used\") \n",
        "        \n",
        "        #trg = [trg sent len, batch size]\n",
        "        #output = [trg sent len, batch size, output dim]\n",
        "        \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        target = target[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg sent len - 1) * batch size]\n",
        "        #output = [(trg sent len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2oEh9PRNc_Dg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, batches, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, b in enumerate(batches):\n",
        "            \n",
        "            source = b['source']\n",
        "            target = b['target']\n",
        "\n",
        "            output = model(source, target, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg sent len, batch size]\n",
        "            #output = [trg sent len, batch size, output dim]\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            target = target[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg sent len - 1) * batch size]\n",
        "            #output = [(trg sent len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_lzhbb9ZKY8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPPrWJuYt3-m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s47vhEzbEul5",
        "colab_type": "code",
        "outputId": "dd366198-e841-462f-8c4a-bafe5370b7c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1385
        }
      },
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 15\n",
        "CLIP = 1\n",
        "\n",
        "INPUT_DIM = len(german['idx2word'])\n",
        "OUTPUT_DIM = len(english['idx2word'])\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "    \n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "bad_epoch_cnt = 0\n",
        "\n",
        "results = {\"hyperparams\": (ENC_EMB_DIM, DEC_HID_DIM),\n",
        "           \"train_loss\": [],\n",
        "           \"valid_loss\": [],\n",
        "           \"train_ppl\": [],\n",
        "           \"valid_ppl\": []}\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    random.shuffle(training_data)\n",
        "    random.shuffle(validation_data)\n",
        "\n",
        "    de_shuffled_td = [td[0] for td in training_data]\n",
        "    en_shuffled_td = [td[1] for td in training_data]\n",
        "\n",
        "    de_shuffled_vd = [vd[0] for vd in validation_data]\n",
        "    en_shuffled_vd = [vd[1] for vd in validation_data]\n",
        "\n",
        "    print(\"Baking batches...\")\n",
        "    train_batches = bake_batches(de_shuffled_td, en_shuffled_td, max_len=max_len_train)\n",
        "    valid_batches = bake_batches(de_shuffled_vd, en_shuffled_vd, max_len=max_len_valid)\n",
        "    print(\"Done.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_batches, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_batches, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        bad_epoch_cnt = 0\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    else:\n",
        "        bad_epoch_cnt += 1  \n",
        "        \n",
        "    print(\"Validation loss has not improved in\", bad_epoch_cnt, \"epochs\")        \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "    \n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['valid_loss'].append(valid_loss)\n",
        "    results['train_ppl'].append(math.exp(train_loss))\n",
        "    results['valid_ppl'].append(math.exp(valid_loss))\n",
        "    \n",
        "    if bad_epoch_cnt >= 3:\n",
        "        print(\"Early Stopping\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 1155.874816 MB used\n",
            "batch: 806 , 2515.460608 MB used\n",
            "batch: 1612 , 2367.776768 MB used\n",
            "batch: 2418 , 2745.581568 MB used\n",
            "batch: 3224 , 5471.469056 MB used\n",
            "Validation loss has not improved in 0 epochs\n",
            "Epoch: 01 | Time: 52m 3s\n",
            "\tTrain Loss: 4.832 | Train PPL: 125.508\n",
            "\t Val. Loss: 5.065 |  Val. PPL: 158.459\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2283.784192 MB used\n",
            "batch: 807 , 2281.941504 MB used\n",
            "batch: 1614 , 2454.07488 MB used\n",
            "batch: 2421 , 2674.765312 MB used\n",
            "Validation loss has not improved in 0 epochs\n",
            "Epoch: 02 | Time: 52m 16s\n",
            "\tTrain Loss: 3.711 | Train PPL:  40.889\n",
            "\t Val. Loss: 4.966 |  Val. PPL: 143.422\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2352.62464 MB used\n",
            "batch: 806 , 2429.545472 MB used\n",
            "batch: 1612 , 2310.562304 MB used\n",
            "batch: 2418 , 2323.360256 MB used\n",
            "batch: 3224 , 5471.473152 MB used\n",
            "Validation loss has not improved in 0 epochs\n",
            "Epoch: 03 | Time: 52m 14s\n",
            "\tTrain Loss: 3.284 | Train PPL:  26.687\n",
            "\t Val. Loss: 4.878 |  Val. PPL: 131.371\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2283.716096 MB used\n",
            "batch: 806 , 2528.177664 MB used\n",
            "batch: 1612 , 2281.648128 MB used\n",
            "batch: 2418 , 2501.796864 MB used\n",
            "batch: 3224 , 5471.443968 MB used\n",
            "Validation loss has not improved in 0 epochs\n",
            "Epoch: 04 | Time: 52m 14s\n",
            "\tTrain Loss: 3.035 | Train PPL:  20.804\n",
            "\t Val. Loss: 4.809 |  Val. PPL: 122.632\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2453.945344 MB used\n",
            "batch: 807 , 2430.284288 MB used\n",
            "batch: 1614 , 2518.623232 MB used\n",
            "batch: 2421 , 2306.659328 MB used\n",
            "Validation loss has not improved in 1 epochs\n",
            "Epoch: 05 | Time: 52m 9s\n",
            "\tTrain Loss: 2.875 | Train PPL:  17.720\n",
            "\t Val. Loss: 4.811 |  Val. PPL: 122.807\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2231.478784 MB used\n",
            "batch: 807 , 2275.638272 MB used\n",
            "batch: 1614 , 2739.337728 MB used\n",
            "batch: 2421 , 2453.175808 MB used\n",
            "batch: 3228 , 6423.021056 MB used\n",
            "Validation loss has not improved in 2 epochs\n",
            "Epoch: 06 | Time: 52m 13s\n",
            "\tTrain Loss: 2.774 | Train PPL:  16.015\n",
            "\t Val. Loss: 4.964 |  Val. PPL: 143.158\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2324.78976 MB used\n",
            "batch: 806 , 2458.02496 MB used\n",
            "batch: 1612 , 2281.92256 MB used\n",
            "batch: 2418 , 2385.348608 MB used\n",
            "batch: 3224 , 5471.501312 MB used\n",
            "Validation loss has not improved in 3 epochs\n",
            "Epoch: 07 | Time: 52m 14s\n",
            "\tTrain Loss: 2.695 | Train PPL:  14.800\n",
            "\t Val. Loss: 4.874 |  Val. PPL: 130.863\n",
            "Early Stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iVBVUJ2qRb3w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results_test_a = {}\n",
        "results_test_b = {}\n",
        "\n",
        "results_test_a['train_loss'] = [4.951, 4.208, 3.919, 3.738, 3.631, 3.566, 3.511     ]\n",
        "results_test_a['valid_loss'] = [5.028, 4.961, 5.049, 5.060, 5.087, 5.102, 5.142    ]\n",
        "results_test_a['train_ppl'] = [141.283, 67.240, 50.341, 42.028, 37.761, 35.362, 33.467]\n",
        "results_test_a['valid_ppl'] = [152.631, 142.753, 155.863, 157.552, 161.846, 164.395, 171.068]\n",
        "\n",
        "results_test_b['train_loss'] = [4.945,  4.226, 3.940, 3.759, 3.651, 3.587, 3.536]\n",
        "results_test_b['valid_loss'] = [5.310, 5.215, 5.187, 5.247, 5.323, 5.303, 5.368]\n",
        "results_test_b['train_ppl'] = [140.527,  68.470, 51.401, 42.904, 38.516, 36.116, 34.334]\n",
        "results_test_b['valid_ppl'] = [202.433, 183.977, 178.941, 189.977, 204.979, 200.840, 214.369]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVMwmNCCLH3W",
        "colab_type": "code",
        "outputId": "a2cec7ae-9d1f-4df1-cd4e-89e3bddbae44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "def sample_net(net, batches):    \n",
        "    preds = []\n",
        "    targets = []\n",
        "    sources = []\n",
        "    sampled_batches = {}\n",
        "    \n",
        "       \n",
        "    with torch.no_grad():\n",
        "        for i, b in enumerate(batches):\n",
        "            print(i, \"out of\", len(batches))\n",
        "            source = b['source']\n",
        "            target = b['target']\n",
        "            \n",
        "#             print(\"L length x N sentences:\", target.shape)\n",
        "            pred = net.forward(source, target, teacher_forcing_ratio=0, run_beam=True)\n",
        "#             print(type(output))\n",
        "#             print(\"L length x N sentences x V vocabulary:\", pred.shape)\n",
        "#             pred = torch.argmax(output, dim=2)\n",
        "            \n",
        "#             print(\"L length x N sentences:\", pred.shape)\n",
        "#             print(\"L length x N sentences:\", target.shape)\n",
        "            \n",
        "            preds.append(pred)\n",
        "            targets.append(target)\n",
        "            sources.append(source)\n",
        "    \n",
        "    sampled_batches['source'] = sources\n",
        "    sampled_batches['target'] = targets\n",
        "    sampled_batches['prediction'] = preds\n",
        "    \n",
        "    return sampled_batches\n",
        "CLIP = 1\n",
        "\n",
        "INPUT_DIM = len(german['idx2word'])\n",
        "OUTPUT_DIM = len(english['idx2word'])\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "model.load_state_dict(torch.load(\"best-model.pt\"))\n",
        "\n",
        "batches = bake_batches(german['dev'], english['dev'])\n",
        "sampled_batches = sample_net(model, batches[0:30])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 out of 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "dpver7T0LIDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_samples(batches):\n",
        "    # Each of the inputs should be a tensor batch of shape L x N\n",
        "    # print(sources.shape)\n",
        "    source = batches['source']\n",
        "    target = batches['target']\n",
        "    prediction = batches['prediction']\n",
        "    \n",
        "    \n",
        "    tgt_out = []\n",
        "    pred_out = []\n",
        "    for b in range(len(batches['source'])):\n",
        "        for sent in range(source[b].shape[1]):            \n",
        "            w = 1\n",
        "            tgt_sent = []\n",
        "            while w < target[b][:,sent].shape[0] and target[b][w,sent].item()!=3:\n",
        "                idx = target[b][w,sent].item()\n",
        "                tgt_sent.append(english['idx2word'][idx])\n",
        "                w+=1\n",
        "            tgt_out.append(tgt_sent)\n",
        "\n",
        "            w = 1\n",
        "            pred_sent = []\n",
        "            while w < prediction[b][:,sent].shape[0] and prediction[b][w,sent].item()!=3:\n",
        "                idx = prediction[b][w,sent].item()\n",
        "                pred_sent.append(english['idx2word'][idx])\n",
        "                w+=1\n",
        "                \n",
        "            pred_out.append(pred_sent)\n",
        "            \n",
        "    with open('target.out', 'w') as f:\n",
        "        for sent in tgt_out:\n",
        "            x = \" \".join(sent)\n",
        "            f.write(x + '\\n')\n",
        "            \n",
        "            \n",
        "    with open('pred.out', 'w') as f:\n",
        "        for sent in pred_out:\n",
        "            y = \" \".join(sent)\n",
        "            f.write(y + '\\n')\n",
        "\n",
        "print_samples(sampled_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "drtkoM5qe9y1",
        "colab_type": "code",
        "outputId": "d525cde5-d3ac-447e-a1ee-50de5c63db81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "!pip install sacrebleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 147302 (delta 0), reused 0 (delta 0), pack-reused 147295\u001b[K\n",
            "Receiving objects: 100% (147302/147302), 129.70 MiB | 19.72 MiB/s, done.\n",
            "Resolving deltas: 100% (113833/113833), done.\n",
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/dc/3ea54419418beb7d0ee0dedd08938519980b951419983a3ddd573c357190/sacrebleu-1.3.2.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Building wheels for collected packages: sacrebleu\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/f1/a9/4cce0ec602e8d195da27bb9b8f6708ec778fbafdbabb097fde\n",
            "Successfully built sacrebleu\n",
            "Installing collected packages: sacrebleu\n",
            "Successfully installed sacrebleu-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "muT6lj3tVLPF",
        "colab_type": "code",
        "outputId": "8438df3e-9721-409b-d15b-e7a1d7e6229a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#!/bin/bash\n",
        "\n",
        "# This is a reference to the gold translations from the dev set\n",
        "REFERENCE_FILE=\"target.out\"\n",
        "\n",
        "# XXX: Change the following line to point to your model's output!\n",
        "TRANSLATED_FILE=\"pred.out\"\n",
        "\n",
        "# The model output is expected to be in a tokenized form. Note, that if you\n",
        "# tokenized your inputs to the model, then simply joined each output token with\n",
        "# whitespace you should get tokenized outputs from your model.\n",
        "# i.e. each output token is separate by whitespace\n",
        "# e.g. \"My model 's output is interesting .\"\n",
        "perl \"mosesdecoder/scripts/tokenizer/detokenizer.perl\" -l en < \"$TRANSLATED_FILE\" > \"$TRANSLATED_FILE.detok\"\n",
        "\n",
        "PARAMS=(\"-tok\" \"intl\" \"-l\" \"de-en\" \"$REFERENCE_FILE\")\n",
        "sacrebleu \"${PARAMS[@]}\" < \"$TRANSLATED_FILE.detok\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: en\n",
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+lang.de-en+numrefs.1+smooth.exp+tok.intl+version.1.3.2 = 28.2 63.7/37.5/22.7/13.5 (BP = 0.964 ratio = 0.965 hyp_len = 13478 ref_len = 13970)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "ogYzCtIcUZH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}