{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6YKpEtz2Wsk",
        "colab_type": "code",
        "outputId": "d0ede568-e4c1-4cb1-a60e-bd9de1a783ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May  3 04:11:09 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    33W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai-duA696vlS",
        "colab_type": "code",
        "outputId": "1c52ffb3-047f-435d-c80b-5b0b2067965a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print('success!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiVfPF4F90za",
        "colab_type": "code",
        "outputId": "ced54a65-e07a-498d-d2b2-d13025c7362e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from progressbar import ProgressBar, Percentage, Bar\n",
        "from google.colab import files\n",
        "import torch, pickle, os, sys, random, time, math, copy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from queue import PriorityQueue\n",
        "\n",
        "########################### ENTIRE DATASET ###########################\n",
        "# german = drive.CreateFile({'id': '1I0_go5RhzEg2CkbpZgx8a6h4hA9qNeyY'})\n",
        "# german.GetContentFile('./german_no_pad_sorted.pickle') \n",
        "# with open('./german_no_pad_sorted.pickle', 'rb') as f_in:\n",
        "#     german = pickle.load(f_in)\n",
        "    \n",
        "# english = drive.CreateFile({'id': '1Tb0aYVfkj3YyXXTxWRkvDO7T__ovng1Q'})\n",
        "# english.GetContentFile('./english_no_pad_sorted.pickle') \n",
        "# with open('./english_no_pad_sorted.pickle', 'rb') as f_in:\n",
        "#     english = pickle.load(f_in)    \n",
        "\n",
        "########################### 5K DATASET ###########################   \n",
        "# german = drive.CreateFile({'id': '10epaM5VzSskc0xkriD0YN_a0Mg3-o1bg'})\n",
        "# german.GetContentFile('./german_no_pad_sorted_5k.pickle') \n",
        "# with open('./german_no_pad_sorted_5k.pickle', 'rb') as f_in:\n",
        "#     german = pickle.load(f_in)\n",
        "\n",
        "# english = drive.CreateFile({'id': '15P0N9sLAIPyxF0Uhgx-01P2s-xOLp6xb'})\n",
        "# english.GetContentFile('./english_no_pad_sorted_5k.pickle') \n",
        "# with open('./english_no_pad_sorted_5k.pickle', 'rb') as f_in:\n",
        "#     english = pickle.load(f_in)\n",
        "\n",
        "########################### 50K DATASET ###########################    \n",
        "# german = drive.CreateFile({'id': '1bNBrVR9TCNXRqlS9kxPa3EffYyZnAfCw'})\n",
        "# german.GetContentFile('./german_no_pad_sorted_50k.pickle') \n",
        "# with open('./german_no_pad_sorted_50k.pickle', 'rb') as f_in:\n",
        "#     german = pickle.load(f_in)\n",
        "    \n",
        "# english = drive.CreateFile({'id': '1XnCWgSLGcp3lCdUK0ciS5m6mwI5L5R2l'})\n",
        "# english.GetContentFile('./english_no_pad_sorted_50k.pickle') \n",
        "# with open('./english_no_pad_sorted_50k.pickle', 'rb') as f_in:\n",
        "#     english = pickle.load(f_in)\n",
        "\n",
        "# ########################### ENTIRE DATASET (WITH UNK) ###########################\n",
        "# german = drive.CreateFile({'id': '1FuoqjfMofNfkIRXBIWDU_cIVV9BBWcs-'})\n",
        "# german.GetContentFile('./german_unk.pickle') \n",
        "# with open('./german_unk.pickle', 'rb') as f_in:\n",
        "#     german = pickle.load(f_in)\n",
        "    \n",
        "# english = drive.CreateFile({'id': '18Q-WRZ2JBwkAJLXwiVuWPfyE-0k76FDA'})\n",
        "# english.GetContentFile('./english_unk.pickle') \n",
        "# with open('./english_unk.pickle', 'rb') as f_in:\n",
        "#     english = pickle.load(f_in)    \n",
        "    \n",
        "########################### ENTIRE DATASET (WITH BPE) ###########################\n",
        "# german = drive.CreateFile({'id': '1KNeZ_WQPUudwlJsg5_kUqPlLjJGBynsn'})\n",
        "# german.GetContentFile('./german_bpe.pickle') \n",
        "# with open('./german_bpe.pickle', 'rb') as f_in:\n",
        "#     german = pickle.load(f_in)\n",
        "    \n",
        "# english = drive.CreateFile({'id': '1ObXLocsuZsVGH3MVxSUdDBpiMlgBqVP7'})\n",
        "# english.GetContentFile('./english_bpe.pickle') \n",
        "# with open('./english_bpe.pickle', 'rb') as f_in:\n",
        "#     english = pickle.load(f_in)     \n",
        "\n",
        "########################### ENTIRE DATASET (WITH BPE) ###########################\n",
        "german = drive.CreateFile({'id': '19_OtUbvPMe7pkhnljnETw2tmyECfBNEu'})\n",
        "german.GetContentFile('./german_bpe_40.pickle') \n",
        "with open('./german_bpe.pickle', 'rb') as f_in:\n",
        "    german = pickle.load(f_in)\n",
        "    \n",
        "english = drive.CreateFile({'id': '1KK9QB8Y5btOKh-tH9ynR3sUtqzwz2U2E'})\n",
        "english.GetContentFile('./english_bpe_40.pickle') \n",
        "with open('./english_bpe.pickle', 'rb') as f_in:\n",
        "    english = pickle.load(f_in)     \n",
        "\n",
        "\n",
        "# for i in range(len(german['train'])):\n",
        "#     german['train'][i] = torch.LongTensor(german['train'][i]).cuda()\n",
        "#     english['train'][i] = torch.LongTensor(english['train'][i]).cuda()\n",
        "    \n",
        "# for i in range(len(german['dev'])):\n",
        "#     german['dev'][i] = torch.LongTensor(german['dev'][i]).cuda()\n",
        "#     english['dev'][i] = torch.LongTensor(english['dev'][i]).cuda()\n",
        "\n",
        "training_data = [[german['train'][i], english['train'][i]] for i in range(len(german['train']))]\n",
        "validation_data = [[german['dev'][i], english['dev'][i]] for i in range(len(german['dev']))]\n",
        "\n",
        "max_len_train = len(max(german['train'], key=len))\n",
        "max_len_valid = len(max(german['dev'], key=len))\n",
        "\n",
        "print(max_len_train, max_len_valid)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "906 281\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxfBDiABDVNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_padded_tensor(batch):\n",
        "    max_len_src = max([len(sent['source']) for sent in batch])  \n",
        "    max_len_trg = max([len(sent['target']) for sent in batch])\n",
        "\n",
        "    for sent in batch:\n",
        "        dif_src = max_len_src - len(sent['source'])\n",
        "        dif_trg = max_len_trg - len(sent['target'])\n",
        "\n",
        "        if dif_src > 0:\n",
        "            pad_list_src = [0 for d in range(dif_src)]\n",
        "            sent['source'].extend(pad_list_src)\n",
        "\n",
        "        if dif_trg > 0:\n",
        "            pad_list_trg = [0 for d in range(dif_trg)]\n",
        "            sent['target'].extend(pad_list_trg)       \n",
        "    \n",
        "    source_sent_len = max_len_src\n",
        "    target_sent_len = max_len_trg\n",
        "    \n",
        "    batch_size = len(batch)\n",
        "    \n",
        "    source = torch.empty((source_sent_len, batch_size)).long().cpu()    \n",
        "    target = torch.empty((target_sent_len, batch_size)).long().cpu()\n",
        "    \n",
        "#     print(batch[0]['target'])\n",
        "    \n",
        "    for i in range(len(batch)):\n",
        "        source[:,i] = torch.tensor(batch[i]['source'])\n",
        "        target[:,i] = torch.tensor(batch[i]['target'])\n",
        "        \n",
        "    padded_tensor = {\"source\": source.to(device),\n",
        "                    \"target\": target.to(device)}\n",
        "    \n",
        "    return padded_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQAoZvykDStp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bake_batches(de, en, batch_size=1700, min_len=3, max_len=768, bucket_step=3):\n",
        "    german = copy.deepcopy(de)\n",
        "    english = copy.deepcopy(en)\n",
        "    \n",
        "    buckets = [[] for i in range(0, max_len, bucket_step)]\n",
        "    bucket_lengths = [0 for i in buckets]\n",
        "    batches = []\n",
        "    \n",
        "    # For every sentence in the dataset, find its corresponding bucket and put it in there, once the bucket\n",
        "    # hits the batch size, ship it off to the batches list\n",
        "    for i in range(len(german)):\n",
        "        sent2sent = {\"source\": german[i],\n",
        "                     \"target\": english[i]}\n",
        "        \n",
        "        # calculate the index of the buckets to put the sentence into, = len(Sentence) // Bucket_step - 1\n",
        "        idx = len(sent2sent['source'])//bucket_step - 1\n",
        "        \n",
        "        if bucket_lengths[idx] + len(sent2sent['source']) > batch_size:\n",
        "            batches.append(to_padded_tensor(buckets[idx][:]))\n",
        "            del buckets[idx][:]\n",
        "            buckets[idx].append(sent2sent)\n",
        "            bucket_lengths[idx] = len(sent2sent['source'])\n",
        "        else:\n",
        "            buckets[idx].append(sent2sent)\n",
        "            bucket_lengths[idx] += len(sent2sent['source'])\n",
        "            \n",
        "\n",
        "    # for any remaining buckets that did not get sent off, send them off to batches\n",
        "    for b in buckets:\n",
        "        if b: # if the list has any value in it\n",
        "            batches.append(to_padded_tensor(b[:]))\n",
        "            del b[:]\n",
        "    \n",
        "    return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZX5hRdP2fHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src sent len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL9QXs3xYoOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, beam=False):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat encoder hidden state src_len times\n",
        "        if not beam:\n",
        "            hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        if not self.training and beam:\n",
        "            hidden = hidden.repeat(encoder_outputs.size(0),encoder_outputs.size(1), 1)\n",
        "        \n",
        "        #hidden = [batch size, src sent len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src sent len, dec hid dim]\n",
        "        \n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        \n",
        "        #energy = [batch size, dec hid dim, src sent len]\n",
        "        \n",
        "        #v = [dec hid dim]\n",
        "        \n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "        \n",
        "        #v = [batch size, 1, dec hid dim]\n",
        "                \n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoiZeHuP4fPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, beam=False):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        if not beam: \n",
        "            input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [sent len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #output = [bsz, output dim]\n",
        "        \n",
        "        return output, hidden.squeeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxmqAe9uKTN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "        \n",
        "    def beam_search(self, target_tensor, decoder_hiddens, encoder_outputs=None):\n",
        "        '''\n",
        "        :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
        "        :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
        "        :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
        "        :return: decoded_batch\n",
        "        '''\n",
        "\n",
        "        beam_width = 4\n",
        "        topk = 4 # how many sentence do you want to generate\n",
        "        decoded_batch = []\n",
        "        # decoding goes sentence by sentence\n",
        "        sentence_length, num_sentences = target_tensor.size()\n",
        "        for idx in range(num_sentences):\n",
        "            if isinstance(decoder_hiddens, tuple):  # LSTM case\n",
        "                decoder_hidden = (decoder_hiddens[0][:,idx, :].unsqueeze(0),decoder_hiddens[1][:,idx, :].unsqueeze(0))\n",
        "            else:\n",
        "                decoder_hidden = decoder_hiddens[idx, :].unsqueeze(0)\n",
        "            encoder_output = encoder_outputs[:,idx, :].unsqueeze(1)\n",
        "\n",
        "            # Start with the start of the sentence token\n",
        "            decoder_input = torch.LongTensor([[2]]).cuda()\n",
        "\n",
        "            # Number of sentence to generate\n",
        "            endnodes = []\n",
        "            number_required = min((topk + 1), topk - len(endnodes))\n",
        "\n",
        "            # starting node -  hidden vector, previous node, word id, logp, length\n",
        "            node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
        "            nodes = PriorityQueue()\n",
        "\n",
        "            # start the queue\n",
        "            nodes.put(node)\n",
        "            qsize = 1\n",
        "\n",
        "            # start beam search\n",
        "            while True:\n",
        "                # give up when decoding takes too long\n",
        "                if qsize > 10000: break\n",
        "\n",
        "                # fetch the best node\n",
        "                n = nodes.get()\n",
        "                decoder_input = n.wordid\n",
        "                decoder_hidden = n.h\n",
        "\n",
        "                if n.wordid.item() == 3 and n.prevNode != None:\n",
        "                    endnodes.append(n)\n",
        "                    # if we reached maximum # of sentences required\n",
        "                    if len(endnodes) >= number_required:\n",
        "                        break\n",
        "                    else:\n",
        "                        continue\n",
        "                \n",
        "                # decode for one step using decoder\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output, True)\n",
        "\n",
        "                # PUT HERE REAL BEAM SEARCH OF TOP\n",
        "                # take softmax first, then log\n",
        "                decoder_output = self.softmax(decoder_output)\n",
        "                \n",
        "                log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
        "                nextnodes = []\n",
        "\n",
        "                for new_k in range(beam_width):\n",
        "                    decoded_t = indexes[0][new_k].view(1, -1)\n",
        "                    log_p = log_prob[0][new_k].item()\n",
        "\n",
        "                    node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp - log_p, n.leng + 1)\n",
        "                    score = -node.eval()\n",
        "                    nextnodes.append(node)\n",
        "\n",
        "                # put them into queue\n",
        "                for i in range(len(nextnodes)):\n",
        "                    nn = nextnodes[i]\n",
        "                    nodes.put(nn)\n",
        "                    # increase qsize\n",
        "                qsize += len(nextnodes) - 1\n",
        "\n",
        "            # choose nbest paths, back trace them\n",
        "            if len(endnodes) == 0:\n",
        "                endnodes = [nodes.get() for _ in range(topk)]\n",
        "\n",
        "            utterances = []\n",
        "            max_prob = -100000\n",
        "            max_sen = False\n",
        "            final_sentence = []\n",
        "            for n in sorted(endnodes):\n",
        "                utterance = []\n",
        "                if max_prob < n.logp:\n",
        "                    max_prob = n.logp\n",
        "                    max_sen = True\n",
        "                utterance.append(n.wordid)\n",
        "                # back trace\n",
        "                while n.prevNode != None:\n",
        "                    n = n.prevNode\n",
        "                    utterance.append(n.wordid)\n",
        "                  \n",
        "                utterance = utterance[::-1]\n",
        "                if max_sen:\n",
        "                    max_sen = False\n",
        "                    final_sentence = utterance\n",
        "                utterances.append(utterance)\n",
        "                \n",
        "                \n",
        "            decoded_batch.append(torch.IntTensor(final_sentence))\n",
        "\n",
        "        return decoded_batch\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5, run_beam=False):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        #trg = [trg sent len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        output = trg[0,:]\n",
        "        \n",
        "        if run_beam:\n",
        "            temp = self.beam_search(trg, hidden ,encoder_outputs)\n",
        "            return temp\n",
        "        # else use greedy\n",
        "        \n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emX_0uUlKC9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamSearchNode(object):\n",
        "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
        "        '''\n",
        "        :param hiddenstate:\n",
        "        :param previousNode:\n",
        "        :param wordId:\n",
        "        :param logProb:\n",
        "        :param length:\n",
        "        '''\n",
        "        self.h = hiddenstate\n",
        "        self.prevNode = previousNode\n",
        "        self.wordid = wordId\n",
        "        self.logp = logProb\n",
        "        self.leng = length\n",
        "\n",
        "    def eval(self, alpha=1.0):\n",
        "        reward = 0\n",
        "        # Add here a function for shaping a reward\n",
        "\n",
        "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward\n",
        "      \n",
        "      \n",
        "    def __lt__(self, other):\n",
        "        return self.logp < other.logp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ejMfHYJc9io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, batches, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, b in enumerate(batches):\n",
        "        \n",
        "        source = b['source']\n",
        "        target = b['target']\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(source, target)\n",
        "        \n",
        "        if i%(len(batches)//4) == 0: \n",
        "            print( \"batch:\", i, \",\", torch.cuda.memory_allocated(device)/1e6, \"MB used\") \n",
        "        \n",
        "        #trg = [trg sent len, batch size]\n",
        "        #output = [trg sent len, batch size, output dim]\n",
        "        \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        target = target[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg sent len - 1) * batch size]\n",
        "        #output = [(trg sent len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oEh9PRNc_Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, batches, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, b in enumerate(batches):\n",
        "            \n",
        "            source = b['source']\n",
        "            target = b['target']\n",
        "\n",
        "            output = model(source, target, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg sent len, batch size]\n",
        "            #output = [trg sent len, batch size, output dim]\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            target = target[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg sent len - 1) * batch size]\n",
        "            #output = [(trg sent len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lzhbb9ZKY8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPPrWJuYt3-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7gKnP6qjXSX",
        "colab_type": "code",
        "outputId": "76815e0a-db02-48c7-87e1-58a56b26c550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def average_sentence_size(german, desired_batch_size=128):\n",
        "    summation = 0\n",
        "    for i in german['train']:\n",
        "        summation += len(i)\n",
        "    avg_tok_sent = summation/len(german['train'])\n",
        "    \n",
        "    print(\"To get an average batch size of\", desired_batch_size, \"Use a batch_size value of:\", int(desired_batch_size*avg_tok_sent))\n",
        "    \n",
        "average_sentence_size(german, desired_batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To get an average batch size of 128 Use a batch_size value of: 3462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47vhEzbEul5",
        "colab_type": "code",
        "outputId": "9d417675-3247-4b4c-b677-a189e9deb69f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1619
        }
      },
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "INPUT_DIM = len(german['idx2word'])\n",
        "OUTPUT_DIM = len(english['idx2word'])\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.4\n",
        "ENC_EMB_DIM = 500\n",
        "DEC_EMB_DIM = 500\n",
        "ENC_HID_DIM = 700\n",
        "DEC_HID_DIM = 700\n",
        "    \n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "bad_epoch_cnt = 0\n",
        "\n",
        "results = {\"hyperparams\": (ENC_EMB_DIM, DEC_HID_DIM),\n",
        "           \"train_loss\": [],\n",
        "           \"valid_loss\": [],\n",
        "           \"train_ppl\": [],\n",
        "           \"valid_ppl\": []}\n",
        "\n",
        "valid_batches = bake_batches(german['dev'], english['dev'], batch_size=3400, max_len=max_len_valid)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    random.shuffle(training_data)\n",
        "    random.shuffle(validation_data)\n",
        "\n",
        "    de_shuffled_td = [td[0] for td in training_data]\n",
        "    en_shuffled_td = [td[1] for td in training_data]\n",
        "\n",
        "    print(\"Baking batches...\")\n",
        "    train_batches = bake_batches(de_shuffled_td, en_shuffled_td, batch_size=2400, max_len=max_len_train)\n",
        "    print(\"Done.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_batches, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_batches, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        bad_epoch_cnt = 0\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    else:\n",
        "        bad_epoch_cnt += 1  \n",
        "        \n",
        "    print(\"Validation loss has not improved in\", bad_epoch_cnt, \"epochs\")        \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "    \n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['valid_loss'].append(valid_loss)\n",
        "    results['train_ppl'].append(math.exp(train_loss))\n",
        "    results['valid_ppl'].append(math.exp(valid_loss))\n",
        "    \n",
        "    if bad_epoch_cnt >= 8:\n",
        "        print(\"Early Stopping\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 1858.74176 MB used\n",
            "batch: 579 , 2701.844992 MB used\n",
            "batch: 1158 , 3121.271296 MB used\n",
            "batch: 1737 , 3299.498496 MB used\n",
            "batch: 2316 , 7497.864704 MB used\n",
            "Validation loss has not improved in 0 epochs\n",
            "Epoch: 01 | Time: 41m 23s\n",
            "\tTrain Loss: 4.772 | Train PPL: 118.150\n",
            "\t Val. Loss: 5.679 |  Val. PPL: 292.733\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2497.376256 MB used\n",
            "batch: 580 , 3268.04224 MB used\n",
            "batch: 1160 , 3545.936384 MB used\n",
            "batch: 1740 , 2751.855104 MB used\n",
            "Validation loss has not improved in 0 epochs\n",
            "Epoch: 02 | Time: 41m 20s\n",
            "\tTrain Loss: 3.735 | Train PPL:  41.892\n",
            "\t Val. Loss: 5.505 |  Val. PPL: 245.902\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2777.792512 MB used\n",
            "batch: 579 , 2913.929216 MB used\n",
            "batch: 1158 , 5013.400064 MB used\n",
            "batch: 1737 , 2420.148736 MB used\n",
            "batch: 2316 , 7517.376512 MB used\n",
            "Validation loss has not improved in 0 epochs\n",
            "Epoch: 03 | Time: 41m 20s\n",
            "\tTrain Loss: 3.403 | Train PPL:  30.058\n",
            "\t Val. Loss: 5.348 |  Val. PPL: 210.286\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2291.974656 MB used\n",
            "batch: 579 , 2275.492352 MB used\n",
            "batch: 1158 , 2561.25696 MB used\n",
            "batch: 1737 , 2499.185152 MB used\n",
            "batch: 2316 , 7517.346304 MB used\n",
            "Validation loss has not improved in 0 epochs\n",
            "Epoch: 04 | Time: 41m 16s\n",
            "\tTrain Loss: 3.197 | Train PPL:  24.448\n",
            "\t Val. Loss: 5.276 |  Val. PPL: 195.608\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2634.50368 MB used\n",
            "batch: 579 , 3400.511488 MB used\n",
            "batch: 1158 , 2432.608256 MB used\n",
            "batch: 1737 , 3173.026816 MB used\n",
            "batch: 2316 , 7517.3632 MB used\n",
            "Validation loss has not improved in 1 epochs\n",
            "Epoch: 05 | Time: 41m 17s\n",
            "\tTrain Loss: 3.063 | Train PPL:  21.382\n",
            "\t Val. Loss: 5.430 |  Val. PPL: 228.066\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2055.247872 MB used\n",
            "batch: 580 , 2529.586688 MB used\n",
            "batch: 1160 , 2114.566656 MB used\n",
            "batch: 1740 , 3839.532544 MB used\n",
            "Validation loss has not improved in 2 epochs\n",
            "Epoch: 06 | Time: 41m 16s\n",
            "\tTrain Loss: 2.961 | Train PPL:  19.313\n",
            "\t Val. Loss: 5.397 |  Val. PPL: 220.645\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 3067.204096 MB used\n",
            "batch: 580 , 3268.061184 MB used\n",
            "batch: 1160 , 5256.386048 MB used\n",
            "batch: 1740 , 2902.602752 MB used\n",
            "Validation loss has not improved in 3 epochs\n",
            "Epoch: 07 | Time: 41m 16s\n",
            "\tTrain Loss: 2.876 | Train PPL:  17.747\n",
            "\t Val. Loss: 5.350 |  Val. PPL: 210.661\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2381.62688 MB used\n",
            "batch: 579 , 2788.376576 MB used\n",
            "batch: 1158 , 2354.3808 MB used\n",
            "batch: 1737 , 2374.444544 MB used\n",
            "batch: 2316 , 7517.371392 MB used\n",
            "Validation loss has not improved in 4 epochs\n",
            "Epoch: 08 | Time: 41m 11s\n",
            "\tTrain Loss: 2.807 | Train PPL:  16.563\n",
            "\t Val. Loss: 5.471 |  Val. PPL: 237.723\n",
            "Baking batches...\n",
            "Done.\n",
            "batch: 0 , 2664.55808 MB used\n",
            "batch: 579 , 2423.1424 MB used\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVBVUJ2qRb3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_test_a = {}\n",
        "results_test_b = {}\n",
        "\n",
        "results_test_a['train_loss'] = [4.951, 4.208, 3.919, 3.738, 3.631, 3.566, 3.511     ]\n",
        "results_test_a['valid_loss'] = [5.028, 4.961, 5.049, 5.060, 5.087, 5.102, 5.142    ]\n",
        "results_test_a['train_ppl'] = [141.283, 67.240, 50.341, 42.028, 37.761, 35.362, 33.467]\n",
        "results_test_a['valid_ppl'] = [152.631, 142.753, 155.863, 157.552, 161.846, 164.395, 171.068]\n",
        "\n",
        "results_test_b['train_loss'] = [4.945,  4.226, 3.940, 3.759, 3.651, 3.587, 3.536]\n",
        "results_test_b['valid_loss'] = [5.310, 5.215, 5.187, 5.247, 5.323, 5.303, 5.368]\n",
        "results_test_b['train_ppl'] = [140.527,  68.470, 51.401, 42.904, 38.516, 36.116, 34.334]\n",
        "results_test_b['valid_ppl'] = [202.433, 183.977, 178.941, 189.977, 204.979, 200.840, 214.369]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVMwmNCCLH3W",
        "colab_type": "code",
        "outputId": "02f49605-3001-4516-b597-734e122a0ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3545
        }
      },
      "source": [
        "def sample_net(net, batches):    \n",
        "    preds = []\n",
        "    targets = []\n",
        "    sources = []\n",
        "    sampled_batches = {}\n",
        "    \n",
        "       \n",
        "    with torch.no_grad():\n",
        "        for i, b in enumerate(batches):\n",
        "            print(i, \"out of\", len(batches))\n",
        "            source = b['source']\n",
        "            target = b['target']\n",
        "            \n",
        "#             print(\"L length x N sentences:\", target.shape)\n",
        "            pred = net.forward(source, target, teacher_forcing_ratio=0, run_beam=False)\n",
        "#             print(type(output))\n",
        "#             print(\"L length x N sentences x V vocabulary:\", pred.shape)\n",
        "            pred = torch.argmax(pred, dim=2)\n",
        "            \n",
        "#             print(\"L length x N sentences:\", pred.shape)\n",
        "#             print(\"L length x N sentences:\", target.shape)\n",
        "            \n",
        "            preds.append(pred)\n",
        "            targets.append(target)\n",
        "            sources.append(source)\n",
        "    \n",
        "    sampled_batches['source'] = sources\n",
        "    sampled_batches['target'] = targets\n",
        "    sampled_batches['prediction'] = preds\n",
        "    \n",
        "    return sampled_batches\n",
        "CLIP = 1\n",
        "\n",
        "INPUT_DIM = len(german['idx2word'])\n",
        "OUTPUT_DIM = len(english['idx2word'])\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "model.load_state_dict(torch.load(\"best-model.pt\"))\n",
        "\n",
        "batches = bake_batches(german['dev'], english['dev'])\n",
        "sampled_batches = sample_net(model, batches)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 out of 196\n",
            "1 out of 196\n",
            "2 out of 196\n",
            "3 out of 196\n",
            "4 out of 196\n",
            "5 out of 196\n",
            "6 out of 196\n",
            "7 out of 196\n",
            "8 out of 196\n",
            "9 out of 196\n",
            "10 out of 196\n",
            "11 out of 196\n",
            "12 out of 196\n",
            "13 out of 196\n",
            "14 out of 196\n",
            "15 out of 196\n",
            "16 out of 196\n",
            "17 out of 196\n",
            "18 out of 196\n",
            "19 out of 196\n",
            "20 out of 196\n",
            "21 out of 196\n",
            "22 out of 196\n",
            "23 out of 196\n",
            "24 out of 196\n",
            "25 out of 196\n",
            "26 out of 196\n",
            "27 out of 196\n",
            "28 out of 196\n",
            "29 out of 196\n",
            "30 out of 196\n",
            "31 out of 196\n",
            "32 out of 196\n",
            "33 out of 196\n",
            "34 out of 196\n",
            "35 out of 196\n",
            "36 out of 196\n",
            "37 out of 196\n",
            "38 out of 196\n",
            "39 out of 196\n",
            "40 out of 196\n",
            "41 out of 196\n",
            "42 out of 196\n",
            "43 out of 196\n",
            "44 out of 196\n",
            "45 out of 196\n",
            "46 out of 196\n",
            "47 out of 196\n",
            "48 out of 196\n",
            "49 out of 196\n",
            "50 out of 196\n",
            "51 out of 196\n",
            "52 out of 196\n",
            "53 out of 196\n",
            "54 out of 196\n",
            "55 out of 196\n",
            "56 out of 196\n",
            "57 out of 196\n",
            "58 out of 196\n",
            "59 out of 196\n",
            "60 out of 196\n",
            "61 out of 196\n",
            "62 out of 196\n",
            "63 out of 196\n",
            "64 out of 196\n",
            "65 out of 196\n",
            "66 out of 196\n",
            "67 out of 196\n",
            "68 out of 196\n",
            "69 out of 196\n",
            "70 out of 196\n",
            "71 out of 196\n",
            "72 out of 196\n",
            "73 out of 196\n",
            "74 out of 196\n",
            "75 out of 196\n",
            "76 out of 196\n",
            "77 out of 196\n",
            "78 out of 196\n",
            "79 out of 196\n",
            "80 out of 196\n",
            "81 out of 196\n",
            "82 out of 196\n",
            "83 out of 196\n",
            "84 out of 196\n",
            "85 out of 196\n",
            "86 out of 196\n",
            "87 out of 196\n",
            "88 out of 196\n",
            "89 out of 196\n",
            "90 out of 196\n",
            "91 out of 196\n",
            "92 out of 196\n",
            "93 out of 196\n",
            "94 out of 196\n",
            "95 out of 196\n",
            "96 out of 196\n",
            "97 out of 196\n",
            "98 out of 196\n",
            "99 out of 196\n",
            "100 out of 196\n",
            "101 out of 196\n",
            "102 out of 196\n",
            "103 out of 196\n",
            "104 out of 196\n",
            "105 out of 196\n",
            "106 out of 196\n",
            "107 out of 196\n",
            "108 out of 196\n",
            "109 out of 196\n",
            "110 out of 196\n",
            "111 out of 196\n",
            "112 out of 196\n",
            "113 out of 196\n",
            "114 out of 196\n",
            "115 out of 196\n",
            "116 out of 196\n",
            "117 out of 196\n",
            "118 out of 196\n",
            "119 out of 196\n",
            "120 out of 196\n",
            "121 out of 196\n",
            "122 out of 196\n",
            "123 out of 196\n",
            "124 out of 196\n",
            "125 out of 196\n",
            "126 out of 196\n",
            "127 out of 196\n",
            "128 out of 196\n",
            "129 out of 196\n",
            "130 out of 196\n",
            "131 out of 196\n",
            "132 out of 196\n",
            "133 out of 196\n",
            "134 out of 196\n",
            "135 out of 196\n",
            "136 out of 196\n",
            "137 out of 196\n",
            "138 out of 196\n",
            "139 out of 196\n",
            "140 out of 196\n",
            "141 out of 196\n",
            "142 out of 196\n",
            "143 out of 196\n",
            "144 out of 196\n",
            "145 out of 196\n",
            "146 out of 196\n",
            "147 out of 196\n",
            "148 out of 196\n",
            "149 out of 196\n",
            "150 out of 196\n",
            "151 out of 196\n",
            "152 out of 196\n",
            "153 out of 196\n",
            "154 out of 196\n",
            "155 out of 196\n",
            "156 out of 196\n",
            "157 out of 196\n",
            "158 out of 196\n",
            "159 out of 196\n",
            "160 out of 196\n",
            "161 out of 196\n",
            "162 out of 196\n",
            "163 out of 196\n",
            "164 out of 196\n",
            "165 out of 196\n",
            "166 out of 196\n",
            "167 out of 196\n",
            "168 out of 196\n",
            "169 out of 196\n",
            "170 out of 196\n",
            "171 out of 196\n",
            "172 out of 196\n",
            "173 out of 196\n",
            "174 out of 196\n",
            "175 out of 196\n",
            "176 out of 196\n",
            "177 out of 196\n",
            "178 out of 196\n",
            "179 out of 196\n",
            "180 out of 196\n",
            "181 out of 196\n",
            "182 out of 196\n",
            "183 out of 196\n",
            "184 out of 196\n",
            "185 out of 196\n",
            "186 out of 196\n",
            "187 out of 196\n",
            "188 out of 196\n",
            "189 out of 196\n",
            "190 out of 196\n",
            "191 out of 196\n",
            "192 out of 196\n",
            "193 out of 196\n",
            "194 out of 196\n",
            "195 out of 196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKV-tHBSrd2N",
        "colab_type": "code",
        "outputId": "c8cad7f0-3a5b-41aa-f0c9-de0f7a5f2529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "! pip install -U sacremoses"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/08/5e38f40491dd6fc40db0faf24de9a96fe213bd5542baafa48ce7fc9b7aa6/sacremoses-0.0.19.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.12.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.28.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c2/17/52c81a9182b3c89376f28aa1500a1088377a37951ed2d78335\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhxKdBKcrNLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sacremoses import MosesDetokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_9R1XGWnA-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_samples(batches):\n",
        "    # Each of the inputs should be a tensor batch of shape L x N\n",
        "    # print(sources.shape)\n",
        "    source = batches['source']\n",
        "    target = batches['target']\n",
        "    prediction = batches['prediction']\n",
        "    \n",
        "    sources_tokenized = []\n",
        "    targets_tokenized = []\n",
        "    preds_tokenized = []\n",
        "    \n",
        "#     md_de = MosesDetokenizer(lang='en')\n",
        "#     md_en = MosesDetokenizer(lang='en')\n",
        "\n",
        "\n",
        "    for b in range(len(batches['source'])):\n",
        "        for sent in range(source[b].shape[1]):\n",
        "            w = 1\n",
        "            src_sent = []\n",
        "            while w < target[b][:,sent].shape[0] and target[b][w,sent].item()!=3:\n",
        "                idx = target[b][w,sent].item()\n",
        "                src_sent.append(german['idx2word'][idx])\n",
        "                w+=1\n",
        "            sources_tokenized.append(src_sent)\n",
        "            \n",
        "            w = 1\n",
        "            tgt_sent = []\n",
        "            while w < target[b][:,sent].shape[0] and target[b][w,sent].item()!=3:\n",
        "                idx = target[b][w,sent].item()\n",
        "                tgt_sent.append(english['idx2word'][idx])\n",
        "                w+=1\n",
        "            targets_tokenized.append(tgt_sent)\n",
        "\n",
        "            w = 1\n",
        "            pred_sent = []\n",
        "            while w < prediction[b][:,sent].shape[0] and prediction[b][w,sent].item()!=3:\n",
        "                idx = prediction[b][w,sent].item()\n",
        "                pred_sent.append(english['idx2word'][idx])\n",
        "                w+=1\n",
        "            preds_tokenized.append(pred_sent)\n",
        "    \n",
        "    with open('source.out', 'w') as f:\n",
        "        for sent in sources_tokenized:\n",
        "            x = \" \".join(sent)\n",
        "            f.write(x + '\\n')\n",
        "    \n",
        "    with open('target.out', 'w') as f:\n",
        "        for sent in targets_tokenized:\n",
        "            x = \" \".join(sent)\n",
        "            f.write(x + '\\n')\n",
        "            \n",
        "    with open('pred.out', 'w') as f:\n",
        "        for sent in preds_tokenized:\n",
        "            y = \" \".join(sent)\n",
        "            f.write(y + '\\n')\n",
        "            \n",
        "print_samples(sampled_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ctCvqqzJm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sed -r -i 's/(@@ )|(@@ ?$)//g' pred.out\n",
        "!sed -r -i 's/(@@ )|(@@ ?$)//g' source.out\n",
        "!sed -r -i 's/(@@ )|(@@ ?$)//g' target.out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drtkoM5qe9y1",
        "colab_type": "code",
        "outputId": "0eecfbbf-ca1f-4996-b950-bde3ed88e71d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "!git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "!pip install sacrebleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 147302 (delta 0), reused 0 (delta 0), pack-reused 147295\u001b[K\n",
            "Receiving objects: 100% (147302/147302), 129.70 MiB | 19.38 MiB/s, done.\n",
            "Resolving deltas: 100% (113833/113833), done.\n",
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/dc/3ea54419418beb7d0ee0dedd08938519980b951419983a3ddd573c357190/sacrebleu-1.3.2.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Building wheels for collected packages: sacrebleu\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/f1/a9/4cce0ec602e8d195da27bb9b8f6708ec778fbafdbabb097fde\n",
            "Successfully built sacrebleu\n",
            "Installing collected packages: sacrebleu\n",
            "Successfully installed sacrebleu-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muT6lj3tVLPF",
        "colab_type": "code",
        "outputId": "51fb1555-8fa7-4731-e048-5c35057b679d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "%%shell\n",
        "#!/bin/bash\n",
        "\n",
        "# This is a reference to the gold translations from the dev set\n",
        "REFERENCE_FILE=\"target.out\"\n",
        "\n",
        "# XXX: Change the following line to point to your model's output!\n",
        "TRANSLATED_FILE=\"pred.out\"\n",
        "\n",
        "# The model output is expected to be in a tokenized form. Note, that if you\n",
        "# tokenized your inputs to the model, then simply joined each output token with\n",
        "# whitespace you should get tokenized outputs from your model.\n",
        "# i.e. each output token is separate by whitespace\n",
        "# e.g. \"My model 's output is interesting .\"\n",
        "perl \"mosesdecoder/scripts/tokenizer/detokenizer.perl\" -l en < \"$TRANSLATED_FILE\" > \"$TRANSLATED_FILE.detok\"\n",
        "\n",
        "PARAMS=(\"-tok\" \"intl\" \"-l\" \"de-en\" \"$REFERENCE_FILE\")\n",
        "sacrebleu \"${PARAMS[@]}\" < \"$TRANSLATED_FILE.detok\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: en\n",
            "BLEU+case.mixed+lang.de-en+numrefs.1+smooth.exp+tok.intl+version.1.3.2 = 11.4 43.4/16.7/7.3/3.2 (BP = 1.000 ratio = 1.001 hyp_len = 171217 ref_len = 171009)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogYzCtIcUZH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}