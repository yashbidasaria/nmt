{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0S3rKkx5XGvc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# from torchtext.datasets import TranslationDataset, Multi30k\n",
    "# from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# import spacy\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from sacremoses import MosesTokenizer\n",
    "usingBPE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0i_EH-Azg-9Y",
    "outputId": "3dfe2096-28f8-40a7-e4a7-79670dd6d287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196884\n",
      "196884\n"
     ]
    }
   ],
   "source": [
    "train_en = []\n",
    "train_de = []\n",
    "with open(\"data/train.en\", \"r\", encoding=\"utf8\") as f:\n",
    "    train_en = f.readlines()\n",
    "with open(\"data/train.de\", \"r\", encoding=\"utf8\") as f:\n",
    "    train_de = f.readlines()\n",
    "with open(\"data/dev.en\", \"r\", encoding=\"utf8\") as f:\n",
    "    dev_en = f.readlines()\n",
    "with open(\"data/dev.de\", \"r\", encoding=\"utf8\") as f:\n",
    "    dev_de = f.readlines()\n",
    "with open(\"data/test.de\", \"r\", encoding=\"utf8\") as f:\n",
    "    test_de = f.readlines()\n",
    "\n",
    "print(len(train_de))\n",
    "print(len(train_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Eym-88ZSiUgF",
    "outputId": "6beb6b05-248a-4fc4-ad62-5f150ce0d49d"
   },
   "outputs": [],
   "source": [
    "# # Get rid of any white space or \\n's\n",
    "# nltk.download('punkt')\n",
    "# start = \"<sos>\"\n",
    "# end = \"<eos>\"\n",
    "# pad = \"<pad>\"\n",
    "# unk = \"<unk>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Moses Tokenizer to tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"<sos>\"\n",
    "end = \"<eos>\"\n",
    "pad = \"<pad>\"\n",
    "unk = \"<unk>\"\n",
    "\n",
    "mt_en = MosesTokenizer(lang='en')\n",
    "mt_de = MosesTokenizer(lang='de')\n",
    "\n",
    "for i, sentence in enumerate(train_en):\n",
    "    tokenized_sentence = mt_en.tokenize(sentence)\n",
    "#     tokenized_sentence = [start] + tokenized_sentence + [end]\n",
    "    train_en[i] = tokenized_sentence\n",
    "    \n",
    "for i, sentence in enumerate(dev_en):\n",
    "    tokenized_sentence = mt_en.tokenize(sentence)\n",
    "#     tokenized_sentence = [start] + tokenized_sentence + [end]\n",
    "    dev_en[i] = tokenized_sentence\n",
    "    \n",
    "for i, sentence in enumerate(train_de):\n",
    "    tokenized_sentence = mt_de.tokenize(sentence)\n",
    "#     tokenized_sentence = [start] + tokenized_sentence + [end]\n",
    "    train_de[i] = tokenized_sentence\n",
    "    \n",
    "for i, sentence in enumerate(dev_de):\n",
    "    tokenized_sentence = mt_de.tokenize(sentence)\n",
    "#     tokenized_sentence = [start] + tokenized_sentence + [end]\n",
    "    dev_de[i] = tokenized_sentence\n",
    "    \n",
    "for i, sentence in enumerate(test_de):\n",
    "    tokenized_sentence = mt_de.tokenize(sentence)\n",
    "#     tokenized_sentence = [start] + tokenized_sentence + [end]\n",
    "    test_de[i] = tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the tokenized data to a file to be processed by byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tokenized/train.tk.en', 'w', encoding=\"utf8\") as f:\n",
    "    for sent in train_en:\n",
    "        x = \" \".join(sent)\n",
    "        f.write(x + '\\n')\n",
    "        \n",
    "with open('data/tokenized/dev.tk.en', 'w', encoding=\"utf8\") as f:\n",
    "    for sent in dev_en:\n",
    "        x = \" \".join(sent)\n",
    "        f.write(x + '\\n')\n",
    "        \n",
    "with open('data/tokenized/train.tk.de', 'w', encoding=\"utf8\") as f:\n",
    "    for sent in train_de:\n",
    "        x = \" \".join(sent)\n",
    "        f.write(x + '\\n')\n",
    "        \n",
    "with open('data/tokenized/dev.tk.de', 'w', encoding=\"utf8\") as f:\n",
    "    for sent in dev_de:\n",
    "        x = \" \".join(sent)\n",
    "        f.write(x + '\\n')\n",
    "        \n",
    "with open('data/tokenized/test.tk.de', 'w', encoding=\"utf8\") as f:\n",
    "    for sent in test_de:\n",
    "        x = \" \".join(sent)\n",
    "        f.write(x + '\\n')\n",
    "        \n",
    "del train_en, train_de, dev_en, dev_de, test_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn byte pair encoding on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd data && subword-nmt learn-joint-bpe-and-vocab --input tokenized/train.tk.de tokenized/train.tk.en -s 35000 -o bpe/codes.txt --write-vocabulary bpe/vocab.de bpe/vocab.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply byte pair encoding with vocabulary filters to all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd data && subword-nmt apply-bpe -c bpe/codes.txt --vocabulary bpe/vocab.de --vocabulary-threshold 50 < tokenized/train.tk.de > bpe/train.BPE.de\n",
    "\n",
    "! cd data && subword-nmt apply-bpe -c bpe/codes.txt --vocabulary bpe/vocab.en --vocabulary-threshold 50 < tokenized/train.tk.en > bpe/train.BPE.en\n",
    " \n",
    "! cd data && subword-nmt apply-bpe -c bpe/codes.txt --vocabulary bpe/vocab.en --vocabulary-threshold 50 < tokenized/dev.tk.en > bpe/dev.BPE.en\n",
    " \n",
    "! cd data && subword-nmt apply-bpe -c bpe/codes.txt --vocabulary bpe/vocab.de --vocabulary-threshold 50 < tokenized/dev.tk.de > bpe/dev.BPE.de\n",
    "\n",
    "! cd data && subword-nmt apply-bpe -c bpe/codes.txt --vocabulary bpe/vocab.de --vocabulary-threshold 50 < tokenized/test.tk.de > bpe/test.BPE.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the tokenized byte pair encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en = []\n",
    "train_de = []\n",
    "dev_en = []\n",
    "dev_de = []\n",
    "test_de = []\n",
    "\n",
    "with open('data/bpe/train.BPE.en','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        sent = []\n",
    "        for word in line.split():\n",
    "            sent.append(word)\n",
    "        train_en.append(sent)\n",
    "        \n",
    "with open('data/bpe/dev.BPE.en','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        sent = []\n",
    "        for word in line.split():\n",
    "            sent.append(word)\n",
    "        dev_en.append(sent)\n",
    "        \n",
    "with open('data/bpe/train.BPE.de','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        sent = []\n",
    "        for word in line.split():\n",
    "            sent.append(word)\n",
    "        train_de.append(sent)\n",
    "        \n",
    "with open('data/bpe/dev.BPE.de','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        sent = []\n",
    "        for word in line.split():\n",
    "            sent.append(word)\n",
    "        dev_de.append(sent)\n",
    "        \n",
    "with open('data/bpe/test.BPE.de','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        sent = []\n",
    "        for word in line.split():\n",
    "            sent.append(word)\n",
    "        test_de.append(sent)\n",
    "        \n",
    "usingBPE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the natural language toolkit to tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNTBocP8iL-n"
   },
   "outputs": [],
   "source": [
    "# # DEPRECATED\n",
    "\n",
    "# start = \"<sos>\"\n",
    "# end = \"<eos>\"\n",
    "# pad = \"<pad>\"\n",
    "# max_length_en = -1\n",
    "# for i, sentence in enumerate(train_en):\n",
    "#     sentence = nltk.word_tokenize(sentence.lower())\n",
    "#     sentence = [start] + sentence\n",
    "#     sentence.append(end)\n",
    "#     max_length_en = len(sentence) if len(sentence) > max_length_en else max_length_en\n",
    "#     train_en[i] = sentence\n",
    "    \n",
    "\n",
    "# max_length_de = -1\n",
    "# for i, sentence in enumerate(train_de):\n",
    "#     sentence = nltk.word_tokenize(sentence.lower())\n",
    "#     sentence = [start] + sentence\n",
    "#     sentence.append(end)\n",
    "#     max_length_de = len(sentence) if len(sentence) > max_length_de else max_length_de\n",
    "#     train_de[i] = sentence\n",
    "    \n",
    "    \n",
    "# max_length_en_dev = -1\n",
    "# for i, sentence in enumerate(dev_en):\n",
    "#     sentence = nltk.word_tokenize(sentence.lower())\n",
    "#     sentence = [start] + sentence\n",
    "#     sentence.append(end)\n",
    "#     max_length_en_dev = len(sentence) if len(sentence) > max_length_en_dev else max_length_en_dev\n",
    "#     dev_en[i] = sentence\n",
    "\n",
    "# max_length_de_dev = -1\n",
    "# for i, sentence in enumerate(dev_de):\n",
    "#     sentence = nltk.word_tokenize(sentence.lower())\n",
    "#     sentence = [start] + sentence\n",
    "#     sentence.append(end)\n",
    "#     max_length_de_dev = len(sentence) if len(sentence) > max_length_de_dev else max_length_de_dev\n",
    "#     dev_de[i] = sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== WARNING ===============\n",
    "\n",
    "## The following code cell deletes a number of sentences from the training set in order to get the dimensionality of the sentences below a certain threshold. The threshold is the value in the inequality of the while loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the longest sentences from the training dataset to decrease the dimensionality of all the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED METHOD\n",
    "\n",
    "# max_idx, en_max_val = max(enumerate(train_en), key=lambda x: len(x[1]))\n",
    "# _, de_max_val = max(enumerate(train_de), key=lambda x: len(x[1]))\n",
    "\n",
    "# del_count = 0\n",
    "# print(max_idx, len(en_max_val), len(de_max_val))\n",
    "\n",
    "# while len(en_max_val) > 200:\n",
    "#     del train_en[max_idx]\n",
    "#     del train_de[max_idx]\n",
    "    \n",
    "#     max_idx, en_max_val = max(enumerate(train_en), key=lambda x: len(x[1]))\n",
    "#     _, de_max_val = max(enumerate(train_de), key=lambda x: len(x[1]))\n",
    "    \n",
    "#     print(max_idx, len(en_max_val), len(de_max_val))\n",
    "    \n",
    "#     del_count += 1\n",
    "    \n",
    "# max_length_en = len(en_max_val)\n",
    "# max_length_de = len(de_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3fH-rhj1HOeE",
    "outputId": "67663a43-7c6e-480f-dd40-e589fe2ea4a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences (training): 196884\n",
      "Number of Sentences (dev): 7883\n",
      "Max length English (training): 843\n",
      "Max length German (training): 858\n",
      "Max length English (dev): 224\n",
      "Max length German (dev): 260\n",
      "\n",
      "Average number of tokens English (training): 23.57 \n",
      "Average number of tokens German (training): 24.32 \n",
      "Average number of tokens English (dev): 23.18 \n",
      "Average number of tokens German (dev): 24.23 \n",
      "Average number of tokens Test Set: 21.59 \n"
     ]
    }
   ],
   "source": [
    "# print(\"Deleted:\", del_count)\n",
    "\n",
    "print(\"Number of Sentences (training):\", len(train_en))\n",
    "print(\"Number of Sentences (dev):\", len(dev_en))\n",
    "\n",
    "print(\"Max length English (training):\", max([len(sent) for sent in train_en]) )\n",
    "print(\"Max length German (training):\", max([len(sent) for sent in train_de]))\n",
    "\n",
    "print(\"Max length English (dev):\", max([len(sent) for sent in dev_en]))\n",
    "print(\"Max length German (dev):\", max([len(sent) for sent in dev_de]))\n",
    "print(\"Max length German (test):\", max([len(sent) for sent in test_de]))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Average number of tokens English (training): { sum([len(sent) for sent in train_en])/len(train_en):.2f} \")\n",
    "print(f\"Average number of tokens German (training): { sum([len(sent) for sent in train_de])/len(train_de):.2f} \") \n",
    "\n",
    "print(f\"Average number of tokens English (dev): { sum([len(sent) for sent in dev_en])/len(dev_en):.2f} \")\n",
    "print(f\"Average number of tokens German (dev): { sum([len(sent) for sent in dev_de])/len(dev_de):.2f} \") \n",
    "\n",
    "print(f\"Average number of tokens Test Set: { sum([len(sent) for sent in test_de])/len(test_de):.2f} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose how large you want the training and validation sets to be\n",
    "## Don't run this cell if you want to use the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 100000 # Number from 1 to 196884\n",
    "VAL_SIZE = 3500 # Number from 1 to 7883\n",
    "\n",
    "idxs_train = list(range(len(train_en)))\n",
    "random.shuffle(idxs_train)\n",
    "idxs_train = idxs_train[0:TRAIN_SIZE]\n",
    "\n",
    "train_en = [train_en[i] for i in idxs_train]\n",
    "train_de = [train_de[i] for i in idxs_train]\n",
    "\n",
    "idxs_val = list(range(len(dev_en)))\n",
    "random.shuffle(idxs_val)\n",
    "idxs_val = idxs_val[0:VAL_SIZE]\n",
    "\n",
    "dev_en = [dev_en[i] for i in idxs_val]\n",
    "dev_de = [dev_de[i] for i in idxs_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a shared vocabulary for both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12388\n"
     ]
    }
   ],
   "source": [
    "vocab ={}\n",
    "\n",
    "vocab['<pad>'] = 0\n",
    "vocab['<unk>'] = 1\n",
    "vocab['<sos>'] = 2\n",
    "vocab['<eos>'] = 3\n",
    "en_inputs = []\n",
    "de_inputs = []\n",
    "\n",
    "en_val = []\n",
    "de_val = []\n",
    "\n",
    "de_test = []\n",
    "\n",
    "for sent in train_en:\n",
    "    idxes = []\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = len(vocab)\n",
    "        idxes.append(vocab[w])\n",
    "    en_inputs.append(idxes)\n",
    "    \n",
    "for sent in dev_en:\n",
    "    idxes = []\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = len(vocab)\n",
    "        idxes.append(vocab[w])\n",
    "    en_val.append(idxes)\n",
    "    \n",
    "for sent in train_de:\n",
    "    idxes = []\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = len(vocab)\n",
    "        idxes.append(vocab[w])\n",
    "    de_inputs.append(idxes)\n",
    "    \n",
    "for sent in dev_de:\n",
    "    idxes = []\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = len(vocab)\n",
    "        idxes.append(vocab[w])\n",
    "    de_val.append(idxes)\n",
    "    \n",
    "for sent in test_de:\n",
    "    idxes = []\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = len(vocab)\n",
    "        idxes.append(vocab[w])\n",
    "    de_test.append(idxes)\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell gets the frequency of words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using BPE data, skipping this cell\n"
     ]
    }
   ],
   "source": [
    "if usingBPE:\n",
    "    print(\"using BPE data, skipping this cell\")\n",
    "    pass\n",
    "else:\n",
    "    en_freq = {'<pad>':100, # there wont be any padding til later, this is just to\n",
    "               '<unk>':2,   # avoid removing the padding token\n",
    "               '<sos>':0,\n",
    "               '<eos>':0}\n",
    "\n",
    "    de_freq = {'<pad>':100, # there wont be any padding til later, this is just to\n",
    "               '<unk>':2,   # avoid removing the padding token\n",
    "               '<sos>':0,\n",
    "               '<eos>':0}\n",
    "\n",
    "    for sent in train_en:\n",
    "        for w in sent:\n",
    "            if w not in en_freq:\n",
    "                en_freq[w] = 1\n",
    "            else:\n",
    "                en_freq[w] += 1\n",
    "\n",
    "    for sent in train_de:\n",
    "        for w in sent:\n",
    "            if w not in de_freq:\n",
    "                de_freq[w] = 1\n",
    "            else:\n",
    "                de_freq[w] += 1\n",
    "\n",
    "    for sent in dev_en:\n",
    "        for w in sent:\n",
    "            if w not in en_freq:\n",
    "                en_freq[w] = 1\n",
    "            else:\n",
    "                en_freq[w] += 1\n",
    "\n",
    "    for sent in dev_de:\n",
    "        for w in sent:\n",
    "            if w not in de_freq:\n",
    "                de_freq[w] = 1\n",
    "            else:\n",
    "                de_freq[w] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a list of uncommon words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using BPE data, skipping this cell\n"
     ]
    }
   ],
   "source": [
    "if usingBPE:\n",
    "    print(\"using BPE data, skipping this cell\")\n",
    "    pass\n",
    "else:\n",
    "    en_uncommon = {}\n",
    "    de_uncommon = {}\n",
    "\n",
    "    for word, freq in en_freq.items():\n",
    "        if freq < 2:\n",
    "            en_uncommon[word] = len(en_uncommon)\n",
    "\n",
    "    for word, freq in de_freq.items():\n",
    "        if freq < 2:\n",
    "            de_uncommon[word] = len(de_uncommon)\n",
    "\n",
    "    if '<unk>' in en_uncommon:\n",
    "        print(\"deleted\")\n",
    "        del en_uncommon['<unk>']\n",
    "\n",
    "    if '<unk>' in de_uncommon:\n",
    "        print(\"deleted\")\n",
    "        del de_uncommon['<unk>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the uncommon words with unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using BPE data, skipping this cell\n"
     ]
    }
   ],
   "source": [
    "if usingBPE:\n",
    "    print(\"using BPE data, skipping this cell\")\n",
    "    pass\n",
    "else:\n",
    "    en_vocab = {}\n",
    "    de_vocab = {}\n",
    "    en_vocab['<pad>'] = 0\n",
    "    en_vocab['<unk>'] = 1 \n",
    "    en_vocab['<sos>'] = 2\n",
    "    en_vocab['<eos>'] = 3\n",
    "\n",
    "    de_vocab['<pad>'] = 0\n",
    "    de_vocab['<unk>'] = 1\n",
    "    de_vocab['<sos>'] = 2\n",
    "    de_vocab['<eos>'] = 3\n",
    "\n",
    "    en_inputs = []\n",
    "    de_inputs = []\n",
    "\n",
    "    en_val = []\n",
    "    de_val = []\n",
    "\n",
    "    for sent in train_en:\n",
    "        en_idxes = []\n",
    "        for i, w in enumerate(sent):\n",
    "            if w in en_uncommon:\n",
    "                sent[i] = '<unk>'\n",
    "                del en_uncommon[w]\n",
    "            elif w not in en_vocab:\n",
    "                en_vocab[w] = len(en_vocab)\n",
    "            en_idxes.append(en_vocab[sent[i]])\n",
    "        en_inputs.append(en_idxes)\n",
    "    del train_en\n",
    "\n",
    "    for sent in dev_en:\n",
    "        en_idxes = []\n",
    "        for i, w in enumerate(sent):\n",
    "            if w in en_uncommon:\n",
    "                sent[i] = '<unk>'\n",
    "                del en_uncommon[w]\n",
    "            elif w not in en_vocab:\n",
    "                en_vocab[w] = len(en_vocab)\n",
    "            en_idxes.append(en_vocab[sent[i]])\n",
    "        en_val.append(en_idxes)\n",
    "    del dev_en\n",
    "\n",
    "    for sent in train_de:\n",
    "        de_idxes = []\n",
    "        for i, w in enumerate(sent):\n",
    "            if w in de_uncommon:\n",
    "                sent[i] = '<unk>'\n",
    "                del de_uncommon[w]\n",
    "            elif w not in de_vocab:\n",
    "                de_vocab[w] = len(de_vocab)\n",
    "            de_idxes.append(de_vocab[sent[i]])\n",
    "        de_inputs.append(de_idxes)\n",
    "    del train_de\n",
    "\n",
    "    for sent in dev_de:\n",
    "        de_idxes = []\n",
    "        for i, w in enumerate(sent):\n",
    "            if w in de_uncommon:\n",
    "                sent[i] = '<unk>'\n",
    "                del de_uncommon[w]\n",
    "            elif w not in de_vocab:\n",
    "                de_vocab[w] = len(de_vocab)\n",
    "            de_idxes.append(de_vocab[sent[i]])\n",
    "        de_val.append(de_idxes)\n",
    "    del dev_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab={}\n",
    "# vocab['<pad>'] = 0\n",
    "# vocab['<unk>'] = 1 \n",
    "# vocab['<sos>'] = 2\n",
    "# vocab['<eos>'] = 3\n",
    "\n",
    "# uncommon = {'then':0, 'thing':1}\n",
    "\n",
    "# sample = ['<sos>', 'the', 'brown', 'cow', 'then', 'jumped', '<eos>']\n",
    "\n",
    "# sidx = []\n",
    "\n",
    "# for i, w in enumerate(sample):\n",
    "#     if w in uncommon:\n",
    "#         sample[i] = '<unk>'\n",
    "#         del uncommon[w]\n",
    "#     elif w not in vocab:\n",
    "#         vocab[w] = len(vocab)\n",
    "#     sidx.append(vocab[sample[i]])  \n",
    "    \n",
    "# print(vocab)\n",
    "# print(uncommon)\n",
    "# print(sample)\n",
    "# print(sidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print examples of words that are infrequently used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23267 of the 58510 english words are only used once\n",
      "71615 of the 129363 german words are only used once\n",
      "cmos\n",
      "super-cooperators\n",
      "dnc\n",
      "weighting\n",
      "leat\n",
      "herblock\n",
      "you.\n",
      "partible\n",
      "trifling\n",
      "cycads\n",
      "kommunikationsgeräte\n",
      "endozytose\n",
      "rosabeth\n",
      "unerschrockene\n",
      "falschlaufenden\n",
      "konsummiert\n",
      "steuersätze\n",
      "ghul\n",
      "wirtschaftskräfte\n",
      "verteilungsseite\n"
     ]
    }
   ],
   "source": [
    "print(len(en_uncommon), \"of the\", len(en_freq), \"english words are only used once\")\n",
    "print(len(de_uncommon), \"of the\", len(de_freq), \"german words are only used once\")\n",
    "\n",
    "en_idx_to_word = dict((v,k) for k,v in en_vocab.items())\n",
    "de_idx_to_word = dict((v,k) for k,v in de_vocab.items())\n",
    "\n",
    "for i in range(10):\n",
    "    idx = en_uncommon[random.randint(0,(len(en_uncommon)))]\n",
    "    print(en_idx_to_word[idx])\n",
    "    \n",
    "for i in range(10):\n",
    "    idx = de_uncommon[random.randint(0,(len(de_uncommon)))]\n",
    "    print(de_idx_to_word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove infrequent words from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_idx_to_word = dict((v,k) for k,v in en_vocab.items())\n",
    "# de_idx_to_word = dict((v,k) for k,v in de_vocab.items())       \n",
    "\n",
    "# print(\"english training\")\n",
    "# for sent in en_inputs:\n",
    "#     for i, idx in enumerate(sent):\n",
    "#         if idx in en_uncommon:\n",
    "#             sent[i] = 1\n",
    "#             del en_idx_to_word[idx]\n",
    "        \n",
    "# print(\"english validation\")\n",
    "# for sent in en_val:\n",
    "#     for i, idx in enumerate(sent):\n",
    "#         if idx in en_uncommon:\n",
    "#             sent[i] = 1\n",
    "#             del en_idx_to_word[idx]\n",
    "\n",
    "# print(\"german training\")\n",
    "# for sent in de_inputs:\n",
    "#     for i, idx in enumerate(sent):\n",
    "#         if idx in de_uncommon:\n",
    "#             sent[i] = 1\n",
    "#             del de_idx_to_word[idx]\n",
    "        \n",
    "# print(\"german validation\")\n",
    "# for sent in de_val:\n",
    "#     for i, idx in enumerate(sent):\n",
    "#         if idx in de_uncommon:\n",
    "#             sent[i] = 1\n",
    "#             del de_idx_to_word[idx]            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Warning ==========\n",
    "\n",
    "## The following code cell sorts the train and validation sets by the length, (smallest to largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inplengths = [len(x) for x in de_inputs]\n",
    "vallengths = [len(x) for x in de_val]\n",
    "\n",
    "sorted_indices_inp = np.argsort(inplengths)\n",
    "sorted_indices_val = np.argsort(vallengths)\n",
    "\n",
    "de_inputs_sorted = []\n",
    "en_inputs_sorted = []\n",
    "de_val_sorted = []\n",
    "en_val_sorted = []\n",
    "\n",
    "for s in sorted_indices_inp:\n",
    "    de_inputs_sorted.append(de_inputs[s])\n",
    "    en_inputs_sorted.append(en_inputs[s])\n",
    "\n",
    "for s in sorted_indices_val:\n",
    "    de_val_sorted.append(de_val[s])\n",
    "    en_val_sorted.append(en_val[s])\n",
    "\n",
    "de_inputs = de_inputs_sorted\n",
    "en_inputs = en_inputs_sorted\n",
    "de_val = de_val_sorted\n",
    "en_val = en_val_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest sentence 768\n"
     ]
    }
   ],
   "source": [
    "print(\"longest sentence\", len(de_inputs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if the sorted data is correct.\n",
    "\n",
    "\n",
    "**en_inputs[112862] and de_inputs[112862] should give:**\n",
    "\n",
    "- <sos> i need your surname . symmetrical objects generally -- spell it for me . <eos> \n",
    "    \n",
    "- <sos> ich brauche ihren nachnamen . symmetrische objekte haben grundsätzlich – bitte buchstabieren sie ihn für mich . <eos> \n",
    "    \n",
    "**en_val[3021] and de_val[3021] should give:**\n",
    "\n",
    "- <sos> but where would be the nearest aed to help this patient ? <eos> \n",
    "    \n",
    "- <sos> aber wo wäre der nächste aed , um diesem patienten zu helfen ? <eos>     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED EXAMPLE\n",
    "\n",
    "# en_idx_to_word = dict((v,k) for k,v in en_vocab.items())\n",
    "# de_idx_to_word = dict((v,k) for k,v in de_vocab.items())\n",
    "\n",
    "# for i in en_inputs[112862]:\n",
    "#     print(en_idx_to_word[i], end=' ')\n",
    "\n",
    "# print(\"\")\n",
    "\n",
    "# for j in de_inputs[112862]:\n",
    "#     print(de_idx_to_word[j], end=' ')\n",
    "    \n",
    "# print(\"\")\n",
    "    \n",
    "# for i in en_val[3021]:\n",
    "#     print(en_idx_to_word[i], end=' ')\n",
    "\n",
    "# print(\"\")\n",
    "\n",
    "# for j in de_val[3021]:\n",
    "#     print(de_idx_to_word[j], end=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Warning ==========\n",
    "\n",
    "## The following code cell adds padding to all of the sentences in both the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED METHOD\n",
    "\n",
    "# for i, sentence in enumerate(en_inputs):\n",
    "#     diff = max_length_en - len(sentence)\n",
    "#     if diff == 0:\n",
    "#         continue\n",
    "#     pad_array = [0]*diff\n",
    "#     sentence = sentence + pad_array\n",
    "#     en_inputs[i] = sentence\n",
    "\n",
    "# for i, sentence in enumerate(de_inputs):\n",
    "#     diff = max_length_de - len(sentence)\n",
    "#     if diff == 0:\n",
    "#         continue\n",
    "#     pad_array = [0]*diff\n",
    "#     sentence = sentence + pad_array\n",
    "#     de_inputs[i] = sentence\n",
    "    \n",
    "# for i, sentence in enumerate(en_val):\n",
    "#     diff = max_length_en_dev - len(sentence)\n",
    "#     if diff == 0:\n",
    "#         continue\n",
    "#     pad_array = [0]*diff\n",
    "#     sentence = sentence + pad_array\n",
    "#     en_val[i] = sentence\n",
    "\n",
    "# for i, sentence in enumerate(de_val):\n",
    "#     diff = max_length_de_dev - len(sentence)\n",
    "#     if diff == 0:\n",
    "#         continue\n",
    "#     pad_array = [0]*diff\n",
    "#     sentence = sentence + pad_array\n",
    "#     de_val[i] = sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== Warning ==========\n",
    "\n",
    "## The following code cell shrinks the number of training sentences from 196k to 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED METHOD\n",
    "# \n",
    "# en_inputs = en_inputs[:50000]\n",
    "# de_inputs = de_inputs[:50000]\n",
    "# print(len(en_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following block saves the data that has been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using BPE data\n",
      "German vocab length 12388\n",
      "English vocab length 12388\n"
     ]
    }
   ],
   "source": [
    "if usingBPE:\n",
    "    print(\"using BPE data\")\n",
    "    \n",
    "    idx_to_word = dict((v,k) for k,v in vocab.items())\n",
    "\n",
    "    en_iwslt = {}\n",
    "    de_iwslt = {}\n",
    "\n",
    "    en_iwslt['idx2word'] = idx_to_word  \n",
    "    de_iwslt['idx2word'] = idx_to_word\n",
    "\n",
    "    en_iwslt['train'] = en_inputs\n",
    "    de_iwslt['train'] = de_inputs\n",
    "\n",
    "    en_iwslt['dev'] = en_val\n",
    "    de_iwslt['dev'] = de_val\n",
    "    \n",
    "    de_iwslt['test'] = de_test\n",
    "\n",
    "    with open('data/processed/english_bpe_40.pickle', 'wb') as handle:\n",
    "        pickle.dump(en_iwslt, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('data/processed/german_bpe_40.pickle', 'wb') as handle:\n",
    "        pickle.dump(de_iwslt, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"German vocab length\", len(de_iwslt['idx2word']))\n",
    "    print(\"English vocab length\", len(en_iwslt['idx2word']))\n",
    "\n",
    "else:\n",
    "    en_idx_to_word = dict((v,k) for k,v in en_vocab.items())\n",
    "    de_idx_to_word = dict((v,k) for k,v in de_vocab.items())\n",
    "\n",
    "    en_iwslt = {}\n",
    "    de_iwslt = {}\n",
    "\n",
    "    en_iwslt['idx2word'] = en_idx_to_word  \n",
    "    de_iwslt['idx2word'] = de_idx_to_word\n",
    "\n",
    "    en_iwslt['train'] = en_inputs\n",
    "    de_iwslt['train'] = de_inputs\n",
    "\n",
    "    en_iwslt['dev'] = en_val\n",
    "    de_iwslt['dev'] = de_val\n",
    "    \n",
    "    de_iwslt['test'] = de_test\n",
    "\n",
    "    with open('data/processed/english_unk_sorted_100k.pickle', 'wb') as handle:\n",
    "        pickle.dump(en_iwslt, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('data/processed/german_unk_sorted_100k.pickle', 'wb') as handle:\n",
    "        pickle.dump(de_iwslt, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"German vocab length\", len(de_iwslt['idx2word']))\n",
    "    print(\"English vocab length\", len(en_iwslt['idx2word']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of how to access some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 13736, 1101, 12731, 5770, 13990, 14, 3]\n",
      "10386\n",
      "14294\n",
      "233\n",
      "<sos> danke . <eos> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(de_val[44])\n",
    "print(len(en_vocab))\n",
    "print(len(de_vocab))\n",
    "print(en_vocab['like'])\n",
    "\n",
    "for i in range(1): \n",
    "    for j in range(len(de_val[i])):\n",
    "        print(de_idx_to_word[de_val[i][j]], end=\" \")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to make sure that the data has been saved correctly (make sure that the filenames match). If you get two Trues, the test is passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKqKmsVwoIp7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('data/processed/english_bpe.pickle', 'rb') as handle:\n",
    "    english = pickle.load(handle)\n",
    "    \n",
    "with open('data/processed/german_bpe.pickle', 'rb') as handle:\n",
    "    german = pickle.load(handle)\n",
    "    \n",
    "print(en_iwslt == english)\n",
    "print(de_iwslt == german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 865, 3]\n"
     ]
    }
   ],
   "source": [
    "print(german['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 865, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocessing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
